{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyObaGMJF91n/qE5GFvYbVP0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/qahtanaa/OnSubGroupFairness/blob/main/Comparison_IR_EqualSize_OS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ajTtcjITxYEH"
      },
      "outputs": [],
      "source": [
        "%reset -f"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install aif360"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "54pkyuCZxf1v",
        "outputId": "34ad6244-1abb-4751-e754-83e7bfb8d63e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: aif360 in /usr/local/lib/python3.10/dist-packages (0.6.1)\n",
            "Requirement already satisfied: numpy>=1.16 in /usr/local/lib/python3.10/dist-packages (from aif360) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from aif360) (1.11.4)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from aif360) (2.0.3)\n",
            "Requirement already satisfied: scikit-learn>=1.0 in /usr/local/lib/python3.10/dist-packages (from aif360) (1.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from aif360) (3.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24.0->aif360) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24.0->aif360) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24.0->aif360) (2024.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0->aif360) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0->aif360) (3.5.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->aif360) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->aif360) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->aif360) (4.53.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->aif360) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->aif360) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->aif360) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->aif360) (3.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=0.24.0->aif360) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from aif360.metrics import utils\n",
        "from scipy.sparse import issparse\n",
        "from sklearn.preprocessing import LabelEncoder, OrdinalEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import random\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sympy import Symbol\n",
        "from sympy.solvers import solve\n",
        "from aif360.datasets import BinaryLabelDataset\n",
        "from aif360.metrics import BinaryLabelDatasetMetric, ClassificationMetric\n",
        "from sklearn.metrics import accuracy_score, balanced_accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
        "from aif360.algorithms.preprocessing import *\n",
        "from aif360.algorithms.preprocessing.optim_preproc_helpers import distortion_functions, opt_tools\n",
        "from aif360.algorithms.inprocessing import *\n",
        "from aif360.algorithms.postprocessing import *\n",
        "import math\n",
        "import itertools\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier"
      ],
      "metadata": {
        "id": "65XxXRvexiQT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "DATASET"
      ],
      "metadata": {
        "id": "4qFei4gZxlV6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_dataset(dataset_path, dataset_type, model):\n",
        "    if dataset_type == 'German':\n",
        "        df = pd.read_csv(dataset_path)\n",
        "        df['age'] = df['age'].apply(lambda age: 1 if age >= 25 else 0)\n",
        "        df['personal_status'] = df['personal_status'].apply(lambda sex: 1 if sex == 'male' else 0)\n",
        "        print(\"German dataset:\")\n",
        "        print(df.head())\n",
        "        sensitive_attributes = ['personal_status','age']\n",
        "        label = 'credit'\n",
        "        privileged = [1, 1]\n",
        "        unprivileged = [0, 0]\n",
        "        favorable_label = 1\n",
        "        unfavorable_label = 2\n",
        "        groups = [\n",
        "                  {'name': 'Male Adult', 'attributes': {'personal_status': 1, 'age': 1}},\n",
        "                  {'name': 'Female Adult', 'attributes': {'personal_status': 0, 'age': 1}},\n",
        "                  {'name': 'Male Young', 'attributes': {'personal_status': 1, 'age': 0}},\n",
        "                  {'name': 'Female Young', 'attributes': {'personal_status': 0, 'age': 0}}\n",
        "              ]\n",
        "        model = model\n",
        "        #num_clust_km = 4\n",
        "\n",
        "    elif dataset_type == 'COMPAS':\n",
        "        df = pd.read_csv(dataset_path)\n",
        "        selected_columns = ['sex', 'age_cat', 'race', 'juv_fel_count', 'juv_misd_count',\n",
        "                            'juv_other_count', 'priors_count', 'c_charge_degree',\n",
        "                            'c_charge_desc', 'two_year_recid']\n",
        "        df = df[selected_columns]\n",
        "        df = df[(df['race'] == 'Caucasian') | (df['race'] == 'African-American')].reset_index(drop=True)\n",
        "        print(\"COMPAS dataset:\")\n",
        "        print(df.head())\n",
        "        sensitive_attributes = ['race','sex']\n",
        "        label = 'two_year_recid'\n",
        "        privileged = ['Caucasian', 'Female']\n",
        "        unprivileged = ['African-American', 'Male']\n",
        "        favorable_label = 0\n",
        "        unfavorable_label = 1\n",
        "        groups = [\n",
        "                  {'name': 'Caucasian Female', 'attributes': {'race': 1, 'sex': 1}},\n",
        "                  {'name': 'Black Female', 'attributes': {'race': 0, 'sex': 1}},\n",
        "                  {'name': 'Causasian Male', 'attributes': {'race': 1, 'sex': 0}},\n",
        "                  {'name': 'Black Male', 'attributes': {'race': 0, 'sex': 0}}\n",
        "              ]\n",
        "        model = model\n",
        "\n",
        "    elif dataset_type == 'Adult':\n",
        "        df = pd.read_csv('/content/raw_adult_dataset.csv', delimiter=';')\n",
        "        df['income'] = df['income'].str.strip().replace({'>50K.': '>50K', '<=50K.': '<=50K'})\n",
        "        df = df.applymap(lambda x: x.strip() if isinstance(x, str) else x)\n",
        "        df.replace('?', np.nan, inplace=True)\n",
        "        df = df.drop(columns=['fnlwgt', 'education-num'])\n",
        "        df = df[(df['race'] == 'White') | (df['race'] == 'Black')].reset_index(drop=True)\n",
        "        print(\"Adult dataset:\")\n",
        "        print(df.head())\n",
        "        sensitive_attributes = ['race','sex']\n",
        "        label = 'income'\n",
        "        privileged = ['White', 'Male']\n",
        "        unprivileged = ['Black', 'Female']\n",
        "        favorable_label = '>50K'\n",
        "        unfavorable_label = '<=50K'\n",
        "        groups = [\n",
        "                  {'name': 'White Male', 'attributes': {'race': 1, 'sex': 1}},\n",
        "                  {'name': 'Black Male', 'attributes': {'race': 0, 'sex': 1}},\n",
        "                  {'name': 'White Female', 'attributes': {'race': 1, 'sex': 0}},\n",
        "                  {'name': 'Black Female', 'attributes': {'race': 0, 'sex': 0}}\n",
        "              ]\n",
        "        model = model\n",
        "\n",
        "    return df, sensitive_attributes, label, privileged, unprivileged, favorable_label, unfavorable_label, groups, model#, num_clust_km"
      ],
      "metadata": {
        "id": "Pv9JWV_UxnJx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##################################################################################\n",
        "df, sensitive_attributes, label, privileged, unprivileged, favorable_label, unfavorable_label, groups, model = preprocess_dataset('/content/raw_adult_dataset.csv', 'Adult', 'Logistic Regression')\n",
        "# = '/content/raw_german_dataset.csv', 'German'\n",
        "# = preprocess_dataset('/content/raw_compas_dataset.csv', 'COMPAS')\n",
        "# = preprocess_dataset('/content/raw_adult_dataset.csv', 'Adult')\n",
        "\n",
        "#model == 'Logistic Regression':\n",
        "#model == 'Random Forest':\n",
        "#model == 'Gradient Boosting':"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jryb6f14xtie",
        "outputId": "683cad97-d5b9-42ec-f2aa-7958d24e8dde"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adult dataset:\n",
            "   age         workclass  education      marital_status         occupation  \\\n",
            "0   39         State-gov  Bachelors       Never-married       Adm-clerical   \n",
            "1   50  Self-emp-not-inc  Bachelors  Married-civ-spouse    Exec-managerial   \n",
            "2   38           Private    HS-grad            Divorced  Handlers-cleaners   \n",
            "3   53           Private       11th  Married-civ-spouse  Handlers-cleaners   \n",
            "4   28           Private  Bachelors  Married-civ-spouse     Prof-specialty   \n",
            "\n",
            "    relationship   race     sex  capital_gain  capital_loss  hours_per_week  \\\n",
            "0  Not-in-family  White    Male          2174             0              40   \n",
            "1        Husband  White    Male             0             0              13   \n",
            "2  Not-in-family  White    Male             0             0              40   \n",
            "3        Husband  Black    Male             0             0              40   \n",
            "4           Wife  Black  Female             0             0              40   \n",
            "\n",
            "  native_country income  \n",
            "0  United-States  <=50K  \n",
            "1  United-States  <=50K  \n",
            "2  United-States  <=50K  \n",
            "3  United-States  <=50K  \n",
            "4           Cuba  <=50K  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "DATA PREPARATION"
      ],
      "metadata": {
        "id": "3azIryqUxwpJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "\n",
        "class DataPreparation():\n",
        "    \"\"\"\n",
        "    ........\n",
        "    \"\"\"\n",
        "    def __init__(self, df, sensitive, label, priv, unpriv, fav, unfav, categorical=[]):\n",
        "        \"\"\"\n",
        "        Construct all necessary attributes for the data preparation.\n",
        "\n",
        "        df : (pandas DataFrame) containing the data\n",
        "        sensitive : (list(str)) specifying the column names of all sensitive features\n",
        "        label : (str) specifying the label column\n",
        "        priv : (list(dicts)) representation of the privileged groups\n",
        "        unpriv : (list(dicts)) representation of the unprivileged groups\n",
        "        fav : (str/int/..) value representing the favorable label\n",
        "        unfav : (str/int/..) value representing the unfavorable label\n",
        "        categorical : (list(str)) (optional) specifying column names of categorical features\n",
        "        \"\"\"\n",
        "        self.df = df\n",
        "        self.sensitive = sensitive\n",
        "        self.label = label\n",
        "        self.priv = priv\n",
        "        self.unpriv = unpriv\n",
        "        self.fav = fav\n",
        "        self.unfav = unfav\n",
        "        self.categorical = categorical\n",
        "\n",
        "    def detect_missing_values(self):\n",
        "        \"\"\"\n",
        "        Detect rows with missing values and remove them from the DataFrame.\n",
        "        \"\"\"\n",
        "        initial_rows = len(self.df)\n",
        "        self.df = self.df.dropna()\n",
        "        removed_rows = initial_rows - len(self.df)\n",
        "\n",
        "        if removed_rows > 0:\n",
        "            print(f\"Detected {removed_rows} rows with missing values. Removed them.\")\n",
        "        else:\n",
        "            print(\"No missing values detected.\")  # pass\n",
        "\n",
        "    def binary_label(self):\n",
        "        \"\"\"\n",
        "        Ensure the decision label and sensitive attributes are encoded as binary, where:\n",
        "        - Favorable label and privileged groups are encoded as 1.\n",
        "        - Unfavorable label and unprivileged groups are encoded as 0.\n",
        "        \"\"\"\n",
        "        if len(self.priv) != 2 or len(self.unpriv) != 2:\n",
        "            raise ValueError(\"Both 'priv' and 'unpriv' must contain exactly two values.\")\n",
        "\n",
        "        number_label_values = self.df[self.label].nunique()\n",
        "        if number_label_values == 2:\n",
        "            print(f\"The '{self.label}' column has only two unique values.\")\n",
        "            self.df.loc[:, self.label] = self.df[self.label].replace([self.unfav, self.fav], [0, 1])\n",
        "        else:\n",
        "            print(f\"The '{self.label}' column does not have exactly two unique values, as it should.\")\n",
        "\n",
        "        # Create mappings for each sensitive attribute\n",
        "        race_mapping = {self.priv[0]: 1, self.unpriv[0]: 0}\n",
        "        sex_mapping = {self.priv[1]: 1, self.unpriv[1]: 0}\n",
        "\n",
        "        # Apply the mappings to the respective columns\n",
        "        self.df.loc[:, self.sensitive[0]] = self.df[self.sensitive[0]].replace(race_mapping)\n",
        "        self.df.loc[:, self.sensitive[1]] = self.df[self.sensitive[1]].replace(sex_mapping)\n",
        "\n",
        "    def find_categorical_attributes(self):\n",
        "        \"\"\"\n",
        "        Identify categorical attributes and encode.\n",
        "        \"\"\"\n",
        "        self.attribute_types = {}\n",
        "\n",
        "        for column in self.df.columns:\n",
        "            if column == 'Group':\n",
        "                continue  # Skip the 'Group' column\n",
        "            elif column in self.categorical:\n",
        "                self.attribute_types[column] = 'Categorical'\n",
        "            elif self.df[column].nunique() == 2:\n",
        "                self.attribute_types[column] = 'Categorical'\n",
        "            else:\n",
        "                num_float = 0\n",
        "                num_text = 0\n",
        "                thresh = 0.99\n",
        "                num_att_in_column = len(self.df[column])\n",
        "\n",
        "                for value in self.df[column]:\n",
        "                    try:\n",
        "                        float(value)\n",
        "                        num_float += 1\n",
        "                    except ValueError:\n",
        "                        num_text += 1\n",
        "\n",
        "                if num_float / num_att_in_column > thresh:\n",
        "                    self.attribute_types[column] = 'Numerical'\n",
        "                else:\n",
        "                    self.attribute_types[column] = 'Categorical'\n",
        "        # Boolean\n",
        "        self.cat_features = []\n",
        "        for attr in self.attribute_types:\n",
        "            self.cat_features.append(self.attribute_types[attr] == 'Categorical')\n",
        "\n",
        "        encoder_dict = dict()\n",
        "        self.columns_categorical = self.df.columns[self.cat_features]\n",
        "\n",
        "        for column in self.columns_categorical:\n",
        "            le = LabelEncoder()\n",
        "            self.df.loc[:, column] = le.fit_transform(self.df[column].values)\n",
        "            mapping = dict(zip(le.classes_, range(len(le.classes_))))\n",
        "            encoder_dict[column] = mapping\n",
        "        print(encoder_dict, 'encoder dict')\n",
        "        self.numerical_features = [not feature for feature in self.cat_features]\n",
        "        self.columns_numerical = self.df.columns[self.numerical_features]\n",
        "\n",
        "        for column in self.columns_numerical:\n",
        "            self.df.loc[:, column] = self.df[column].astype(float)\n",
        "\n",
        "        return self.attribute_types, self.cat_features, self.numerical_features\n",
        "\n",
        "    def create_group_column(self):\n",
        "        \"\"\"\n",
        "        Create a 'Group' column in the DataFrame based on protected attributes, privileged/unprivileged conditions, and label.\n",
        "        \"\"\"\n",
        "        group_combinations = pd.MultiIndex.from_product([self.df[sensitive].unique() for sensitive in self.sensitive] + [self.df[self.label].unique()], names=self.sensitive + [self.label])\n",
        "        print(list(enumerate(group_combinations)))\n",
        "        # Create a mapping between group combinations and their corresponding numbers\n",
        "        group_mapping = {group: idx for idx, group in enumerate(group_combinations)}\n",
        "        reverse_group_mapping = {idx: group for group, idx in group_mapping.items()}\n",
        "        self.df['Group'] = pd.MultiIndex.from_frame(self.df[self.sensitive + [self.label]]).map(group_mapping)\n",
        "\n",
        "        return reverse_group_mapping\n",
        "\n",
        "    def train_test_split(self):\n",
        "        X = self.df.loc[:, self.df.columns != self.label]\n",
        "        y = self.df[self.label]\n",
        "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(X, y, test_size=0.3, shuffle=True, stratify=self.df['Group'])  # , random_state=42\n",
        "\n",
        "        return self.X_train, self.y_train, self.X_test, self.y_test\n",
        "\n",
        "    def standardization_numerical(self):\n",
        "        train_dataset_numerical = self.X_train[self.columns_numerical]\n",
        "        test_dataset_numerical = self.X_test[self.columns_numerical]\n",
        "\n",
        "        scaler = StandardScaler().fit(train_dataset_numerical)\n",
        "        train_dataset_scaled_numerical = scaler.transform(train_dataset_numerical)\n",
        "        test_dataset_scaled_numerical = scaler.transform(test_dataset_numerical)\n",
        "\n",
        "        self.X_train.loc[:, self.columns_numerical] = train_dataset_scaled_numerical\n",
        "        self.X_test.loc[:, self.columns_numerical] = test_dataset_scaled_numerical\n",
        "\n",
        "        self.X_train = pd.concat([self.X_train, self.y_train], axis=1)\n",
        "        self.X_test = pd.concat([self.X_test, self.y_test], axis=1)\n",
        "        return self.X_train, self.X_test\n",
        "\n",
        "    def prepare(self):\n",
        "        \"\"\"\n",
        "        Perform all preprocessing steps.\n",
        "        \"\"\"\n",
        "        self.detect_missing_values()\n",
        "        self.binary_label()\n",
        "        self.find_categorical_attributes()\n",
        "        self.create_group_column()\n",
        "        self.train_test_split()\n",
        "        self.standardization_numerical()\n",
        "        return self\n"
      ],
      "metadata": {
        "id": "bIe3aEJ-xz8n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##################################################################################\n",
        "#use the DataPreparation class to preprocess the dataframe\n",
        "data_prep = DataPreparation(df, sensitive_attributes, label, privileged, unprivileged, favorable_label, unfavorable_label)\n",
        "data_prep.prepare()\n",
        "data_prep.df = data_prep.df.reset_index(drop=True)\n",
        "X_train, X_test = data_prep.X_train, data_prep.X_test\n",
        "attribute_types = data_prep.attribute_types\n",
        "cat_features = data_prep.cat_features\n",
        "numerical_features = data_prep.numerical_features\n",
        "reverse_group_mapping = data_prep.create_group_column()\n",
        "#theoretical_num_groups = len(reverse_group_mapping)\n",
        "X_train = X_train.reset_index(drop=True)\n",
        "print(X_train.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ng5wibdxx4-8",
        "outputId": "5b2ed044-c46b-4cd8-93ab-d7a04ba092b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected 3316 rows with missing values. Removed them.\n",
            "The 'income' column has only two unique values.\n",
            "{'workclass': {'Federal-gov': 0, 'Local-gov': 1, 'Private': 2, 'Self-emp-inc': 3, 'Self-emp-not-inc': 4, 'State-gov': 5, 'Without-pay': 6}, 'education': {'10th': 0, '11th': 1, '12th': 2, '1st-4th': 3, '5th-6th': 4, '7th-8th': 5, '9th': 6, 'Assoc-acdm': 7, 'Assoc-voc': 8, 'Bachelors': 9, 'Doctorate': 10, 'HS-grad': 11, 'Masters': 12, 'Preschool': 13, 'Prof-school': 14, 'Some-college': 15}, 'marital_status': {'Divorced': 0, 'Married-AF-spouse': 1, 'Married-civ-spouse': 2, 'Married-spouse-absent': 3, 'Never-married': 4, 'Separated': 5, 'Widowed': 6}, 'occupation': {'Adm-clerical': 0, 'Armed-Forces': 1, 'Craft-repair': 2, 'Exec-managerial': 3, 'Farming-fishing': 4, 'Handlers-cleaners': 5, 'Machine-op-inspct': 6, 'Other-service': 7, 'Priv-house-serv': 8, 'Prof-specialty': 9, 'Protective-serv': 10, 'Sales': 11, 'Tech-support': 12, 'Transport-moving': 13}, 'relationship': {'Husband': 0, 'Not-in-family': 1, 'Other-relative': 2, 'Own-child': 3, 'Unmarried': 4, 'Wife': 5}, 'race': {0: 0, 1: 1}, 'sex': {0: 0, 1: 1}, 'native_country': {'Cambodia': 0, 'Canada': 1, 'China': 2, 'Columbia': 3, 'Cuba': 4, 'Dominican-Republic': 5, 'Ecuador': 6, 'El-Salvador': 7, 'England': 8, 'France': 9, 'Germany': 10, 'Greece': 11, 'Guatemala': 12, 'Haiti': 13, 'Holand-Netherlands': 14, 'Honduras': 15, 'Hong': 16, 'Hungary': 17, 'India': 18, 'Iran': 19, 'Ireland': 20, 'Italy': 21, 'Jamaica': 22, 'Japan': 23, 'Mexico': 24, 'Nicaragua': 25, 'Outlying-US(Guam-USVI-etc)': 26, 'Peru': 27, 'Philippines': 28, 'Poland': 29, 'Portugal': 30, 'Puerto-Rico': 31, 'Scotland': 32, 'South': 33, 'Taiwan': 34, 'Thailand': 35, 'Trinadad&Tobago': 36, 'United-States': 37, 'Vietnam': 38, 'Yugoslavia': 39}, 'income': {0: 0, 1: 1}} encoder dict\n",
            "[(0, (1, 1, 0)), (1, (1, 1, 1)), (2, (1, 0, 0)), (3, (1, 0, 1)), (4, (0, 1, 0)), (5, (0, 1, 1)), (6, (0, 0, 0)), (7, (0, 0, 1))]\n",
            "[(0, (1, 1, 0)), (1, (1, 1, 1)), (2, (1, 0, 0)), (3, (1, 0, 1)), (4, (0, 1, 0)), (5, (0, 1, 1)), (6, (0, 0, 0)), (7, (0, 0, 1))]\n",
            "        age workclass education marital_status occupation relationship race  \\\n",
            "0 -0.950734         2        15              2         10            0    1   \n",
            "1 -0.346115         2        15              2          3            0    1   \n",
            "2  0.636391         2         8              2          0            0    0   \n",
            "3  0.031772         5         9              2          9            0    1   \n",
            "4 -0.724002         2        15              4          3            1    1   \n",
            "\n",
            "  sex  capital_gain  capital_loss  hours_per_week native_country  Group income  \n",
            "0   1     -0.146986     -0.217786       -0.080495             37      0      0  \n",
            "1   1     -0.146986     -0.217786        1.168941             37      0      0  \n",
            "2   1     -0.146986     -0.217786       -0.080495             37      4      0  \n",
            "3   1     -0.146986     -0.217786        0.335983             37      1      1  \n",
            "4   1     -0.146986     -0.217786        0.335983             37      0      0  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-83-379c487b634e>:129: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  self.df['Group'] = pd.MultiIndex.from_frame(self.df[self.sensitive + [self.label]]).map(group_mapping)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "grouped_counts = X_train.groupby(sensitive_attributes + [label]).size().reset_index(name='count')\n",
        "\n",
        "# Print the counts\n",
        "print(grouped_counts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BDPBE5KvTroZ",
        "outputId": "054d9b42-975b-498f-c4ce-8624a016694d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   race  sex  income  count\n",
            "0     0    0       0   1371\n",
            "1     0    0       1     88\n",
            "2     0    1       0   1215\n",
            "3     0    1       1    286\n",
            "4     1    0       0   7299\n",
            "5     1    0       1   1019\n",
            "6     1    1       0  12787\n",
            "7     1    1       1   6126\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define sensitive attribute and label columns\n",
        "sensitive_attr_0 = sensitive_attributes[0]\n",
        "sensitive_attr_1 = sensitive_attributes[1]\n",
        "\n",
        "# Calculate the number of ones and zeros for each group\n",
        "num_group_11_ones = X_train[(X_train[sensitive_attr_0] == 1) &\n",
        "                            (X_train[sensitive_attr_1] == 1) &\n",
        "                            (X_train[label] == 1)].shape[0]\n",
        "\n",
        "num_group_11_zeros = X_train[(X_train[sensitive_attr_0] == 1) &\n",
        "                             (X_train[sensitive_attr_1] == 1) &\n",
        "                             (X_train[label] == 0)].shape[0]\n",
        "\n",
        "num_group_10_ones = X_train[(X_train[sensitive_attr_0] == 1) &\n",
        "                            (X_train[sensitive_attr_1] == 0) &\n",
        "                            (X_train[label] == 1)].shape[0]\n",
        "\n",
        "num_group_10_zeros = X_train[(X_train[sensitive_attr_0] == 1) &\n",
        "                             (X_train[sensitive_attr_1] == 0) &\n",
        "                             (X_train[label] == 0)].shape[0]\n",
        "\n",
        "num_group_01_ones = X_train[(X_train[sensitive_attr_0] == 0) &\n",
        "                            (X_train[sensitive_attr_1] == 1) &\n",
        "                            (X_train[label] == 1)].shape[0]\n",
        "\n",
        "num_group_01_zeros = X_train[(X_train[sensitive_attr_0] == 0) &\n",
        "                             (X_train[sensitive_attr_1] == 1) &\n",
        "                             (X_train[label] == 0)].shape[0]\n",
        "\n",
        "num_group_00_ones = X_train[(X_train[sensitive_attr_0] == 0) &\n",
        "                            (X_train[sensitive_attr_1] == 0) &\n",
        "                            (X_train[label] == 1)].shape[0]\n",
        "\n",
        "num_group_00_zeros = X_train[(X_train[sensitive_attr_0] == 0) &\n",
        "                             (X_train[sensitive_attr_1] == 0) &\n",
        "                             (X_train[label] == 0)].shape[0]\n",
        "\n",
        "# Calculate imbalance ratios\n",
        "imbalance_ratio_11 = num_group_11_ones / num_group_11_zeros if num_group_11_zeros != 0 else float('inf')\n",
        "imbalance_ratio_10 = num_group_10_ones / num_group_10_zeros if num_group_10_zeros != 0 else float('inf')\n",
        "imbalance_ratio_01 = num_group_01_ones / num_group_01_zeros if num_group_01_zeros != 0 else float('inf')\n",
        "imbalance_ratio_00 = num_group_00_ones / num_group_00_zeros if num_group_00_zeros != 0 else float('inf')\n",
        "\n",
        "# Print imbalance ratios\n",
        "print(f\"Imbalance ratio for group (1, 1): {imbalance_ratio_11}\")\n",
        "print(f\"Imbalance ratio for group (1, 0): {imbalance_ratio_10}\")\n",
        "print(f\"Imbalance ratio for group (0, 1): {imbalance_ratio_01}\")\n",
        "print(f\"Imbalance ratio for group (0, 0): {imbalance_ratio_00}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eGTH5aCPl7y8",
        "outputId": "c67bc2e7-a939-41f0-ca91-dff2d9e7d4c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Imbalance ratio for group (1, 1): 0.4790803159458825\n",
            "Imbalance ratio for group (1, 0): 0.13960816550212357\n",
            "Imbalance ratio for group (0, 1): 0.2353909465020576\n",
            "Imbalance ratio for group (0, 0): 0.06418672501823487\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# #################################################################################\n",
        "# num_privileged_ones = X_train[(X_train[sensitive_attributes[0]] == 1) &\n",
        "#                               (X_train[sensitive_attributes[1]] == 1) &\n",
        "#                               (X_train[label] == 1)].shape[0]\n",
        "\n",
        "# num_privileged_zeros = X_train[(X_train[sensitive_attributes[0]]) &\n",
        "#                                (X_train[sensitive_attributes[1]] == 1) &\n",
        "#                                (X_train[label] == 0)].shape[0]\n",
        "\n",
        "# # Calculating the ratio of the most privileged class\n",
        "# total_ratio = num_privileged_ones / num_privileged_zeros if num_privileged_zeros != 0 else float('inf')  # Avoid division by zero\n",
        "\n",
        "# print(f\"Ratio of most privileged class: {total_ratio}\")"
      ],
      "metadata": {
        "id": "IJSGh8H2yCng"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#################################################################################\n",
        "## Save the 'Group' column from X_train\n",
        "subgroup_column_train = X_train['Group']\n",
        "subgroup_column_test = X_test['Group']\n",
        "\n",
        "# Drop the 'Group' column from X_train\n",
        "X_train = X_train.drop(columns=['Group'])\n",
        "X_test = X_test.drop(columns=['Group'])"
      ],
      "metadata": {
        "id": "_JA0r62uyFHl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#distances = gower.gower_matrix(??, os_df, cat_features=cat_features)"
      ],
      "metadata": {
        "id": "LoTMmXmFyOFW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gower"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NuN9SabF7fTj",
        "outputId": "7790b2ad-caa2-4ae2-dca5-71667a29ed90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gower in /usr/local/lib/python3.10/dist-packages (0.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from gower) (1.25.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from gower) (1.11.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "METHOD 3 - DBSCAN \\\\\n",
        "for each group in the dataset: \\\\\n",
        "-cluster the group in order to find the points that are considered Core, Borderline, Noise by the DBSCAN algorithm \\\\\n",
        "-oversample each group using the Borderline points, to reach the imbalance ratio of the most priviledged group\n",
        "\n"
      ],
      "metadata": {
        "id": "WGnBXmRmeW12"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# find the eps\n",
        "from gower import gower_matrix\n",
        "from sklearn.cluster import DBSCAN\n",
        "import math\n",
        "\n",
        "# # Filter the dataset for one specific combination of sensitive attributes and labels\n",
        "# filtered_data = X_train[(X_train[sensitive_attributes[0]] == 1) & (X_train[sensitive_attributes[1]] == 0) & (X_train[label] == 1)]\n",
        "\n",
        "# # Calculate Gower distance matrix\n",
        "# distance_matrix = gower_matrix(filtered_data, cat_features=cat_features)\n",
        "\n",
        "# eps = 0.16\n",
        "# # German: group (1,1,1), eps = 0.15 3 clusters\n",
        "# #round(math.log(len(filtered_data)))\n",
        "# # DBSCAN clustering\n",
        "# dbscan = DBSCAN(eps=eps, min_samples=round(math.log(len(filtered_data))), metric='precomputed')  # Set appropriate values for eps and min_samples\n",
        "# clusters = dbscan.fit_predict(distance_matrix)\n",
        "\n",
        "# # Assign cluster labels to dataframe\n",
        "# filtered_data['cluster'] = clusters\n",
        "\n",
        "# print(round(math.log(len(filtered_data))))\n",
        "# # Display the number of samples in each cluster\n",
        "# cluster_counts = filtered_data.groupby(['cluster']).size()\n",
        "# print(\"Number of samples in each cluster:\")\n",
        "# print(cluster_counts)\n",
        "\n",
        "# # Get cluster labels\n",
        "# labels = dbscan.labels_\n",
        "\n",
        "# # Get core samples\n",
        "# core_samples_mask = np.zeros_like(labels, dtype=bool)\n",
        "# core_samples_mask[dbscan.core_sample_indices_] = True\n",
        "\n",
        "# # Identify core, border, and noise points\n",
        "# core_points = filtered_data[core_samples_mask]\n",
        "# border_points = filtered_data[~core_samples_mask & (labels != -1)]\n",
        "# noise_points = filtered_data[labels == -1]\n",
        "\n",
        "# # For example, you can print the number of points in each category\n",
        "# print(\"Number of core points:\", len(core_points))\n",
        "# print(\"Number of border points:\", len(border_points))\n",
        "# print(\"Number of noise points:\", len(noise_points))\n"
      ],
      "metadata": {
        "id": "jLQphAgzhSGO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Function to find the optimal epsilon\n",
        "# def find_optimal_epsilonn(filtered_data, cat_features, min_samples, eps_step=0.001, eps_min=0.01, eps_max=1.1):\n",
        "#     distance_matrix = gower_matrix(filtered_data, cat_features=cat_features)\n",
        "\n",
        "#     def cluster_count(eps):\n",
        "#         dbscan = DBSCAN(eps=eps, min_samples=min_samples, metric='precomputed')\n",
        "#         labels = dbscan.fit_predict(distance_matrix)\n",
        "#         unique_labels = np.unique(labels)\n",
        "#         n_clusters = len(unique_labels) #- (1 if -1 in unique_labels else 0)\n",
        "#         return n_clusters, unique_labels\n",
        "\n",
        "#     # Binary search for optimal epsilon\n",
        "#     while eps_max - eps_min > eps_step:\n",
        "#         eps_mid = (eps_min + eps_max) / 2\n",
        "#         n_clusters_mid, labels_mid = cluster_count(eps_mid)\n",
        "#         print(eps_mid, n_clusters_mid, labels_mid, 'mids')\n",
        "\n",
        "#         if n_clusters_mid == 1:\n",
        "#             if -1 in labels_mid:\n",
        "#                 eps_min = eps_mid  # Only noise points, increase epsilon\n",
        "#             else:\n",
        "#                 eps_max = eps_mid  # Only core points, decrease epsilon\n",
        "#         elif n_clusters_mid > 2:\n",
        "#             eps_min = eps_mid  # More than two clusters, increase epsilon\n",
        "#         else:\n",
        "#             eps_max = eps_mid  # Exactly two clusters, continue search to fine-tune\n",
        "#         print(eps_min, eps_max, 'min, max')\n",
        "#     return eps_max\n",
        "\n",
        "# # Filter the dataset for one specific combination of sensitive attributes and labels\n",
        "# filtered_data = X_train[(X_train[sensitive_attributes[0]] == 0) &\n",
        "#                         (X_train[sensitive_attributes[1]] == 1) &\n",
        "#                         (X_train[label] == 1)]\n",
        "\n",
        "# # Calculate min_samples\n",
        "# min_samples = round(math.log(len(filtered_data)))\n",
        "\n",
        "# # Find the optimal epsilon\n",
        "# optimal_eps = find_optimal_epsilonn(filtered_data, cat_features, min_samples)\n",
        "\n",
        "# # DBSCAN clustering with the optimal epsilon\n",
        "# distance_matrix = gower_matrix(filtered_data, cat_features=cat_features)\n",
        "# dbscan = DBSCAN(eps=optimal_eps, min_samples=min_samples, metric='precomputed')\n",
        "# clusters = dbscan.fit_predict(distance_matrix)\n",
        "\n",
        "# # Assign cluster labels to dataframe\n",
        "# filtered_data['cluster'] = clusters\n",
        "\n",
        "# # Display the number of samples in each cluster\n",
        "# cluster_counts = filtered_data.groupby(['cluster']).size()\n",
        "# print(\"Number of samples in each cluster:\")\n",
        "# print(cluster_counts)\n",
        "\n",
        "# # Get cluster labels\n",
        "# labels = dbscan.labels_\n",
        "\n",
        "# # Get core samples\n",
        "# core_samples_mask = np.zeros_like(labels, dtype=bool)\n",
        "# core_samples_mask[dbscan.core_sample_indices_] = True\n",
        "\n",
        "# # Identify core, border, and noise points\n",
        "# core_points = filtered_data[core_samples_mask]\n",
        "# border_points = filtered_data[~core_samples_mask & (labels != -1)]\n",
        "# noise_points = filtered_data[labels == -1]\n",
        "\n",
        "# # Print the number of points in each category\n",
        "# print(\"Optimal epsilon:\", optimal_eps)\n",
        "# print(\"Number of core points:\", len(core_points))\n",
        "# print(\"Number of border points:\", len(border_points))\n",
        "# print(\"Number of noise points:\", len(noise_points))\n"
      ],
      "metadata": {
        "id": "9L_tLlmuhPxK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from gower import gower_matrix\n",
        "# from sklearn.cluster import DBSCAN\n",
        "# import numpy as np\n",
        "# import matplotlib.pyplot as plt\n",
        "# from sklearn.neighbors import NearestNeighbors\n",
        "\n",
        "# # Compute distances to nearest neighbors\n",
        "# k = round(math.log(len(filtered_data)))\n",
        "# nbrs = NearestNeighbors(n_neighbors=k, metric='precomputed').fit(distance_matrix)\n",
        "# distances, _ = nbrs.kneighbors(distance_matrix)\n",
        "\n",
        "# # Compute reachability distances\n",
        "# reachability_distances = np.mean(distances[:, 1:], axis=1)\n",
        "\n",
        "# # Sort reachability distances in ascending order\n",
        "# sorted_distances = np.sort(reachability_distances)\n",
        "\n",
        "# # Plot reachability distances\n",
        "# plt.figure(figsize=(6, 4))\n",
        "# plt.plot(sorted_distances)\n",
        "# plt.title('Reachability Plot')\n",
        "# plt.xlabel('Data Points (Sorted)')\n",
        "# plt.ylabel('Reachability Distance')\n",
        "# plt.grid(True)\n",
        "# plt.show()\n"
      ],
      "metadata": {
        "id": "lmKU7Nidloo3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Compute distances to K-nearest neighbors\n",
        "# k = round(math.log(len(filtered_data))) # Choose the value of K\n",
        "# nbrs = NearestNeighbors(n_neighbors=k, metric='precomputed').fit(distance_matrix)\n",
        "# distances, _ = nbrs.kneighbors(distance_matrix)\n",
        "\n",
        "# # Sort distances\n",
        "# sorted_distances = np.sort(distances[:, -1])\n",
        "\n",
        "# # Plot K-distance graph\n",
        "# plt.plot(range(len(filtered_data)), sorted_distances)\n",
        "# plt.xlabel('Data Points')\n",
        "# plt.ylabel('Distance to Kth Nearest Neighbor')\n",
        "# plt.title('K-distance Graph')\n",
        "# plt.show()\n"
      ],
      "metadata": {
        "id": "aHGrlYA-lujp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "BfdqxcX3wqQz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def oversample_groups(X_train, cat_features, custom_smote, group_column_train,sensitive_attributes,label, reverse_group_mapping):\n",
        "    \"\"\"\n",
        "    Function to oversample multiple groups automatically based on group labels.\n",
        "\n",
        "    Parameters:\n",
        "    - X_train: Preprocessed training dataset.\n",
        "    - cat_features: List indicating categorical features.\n",
        "    - custom_smote: Custom SMOTE function to be used.\n",
        "    - group_column_train: Column containing the group label for each instance.\n",
        "    - total_ratio: Desired ratio of positive to negative labels.\n",
        "    - reverse_group_mapping: Mapping of groups to sensitive attributes and labels.\n",
        "\n",
        "    Returns:\n",
        "    - synthetic_samples_matrix: Matrix containing all generated synthetic samples.\n",
        "    - synthetic_samples_group: Array of group labels for the synthetic samples.\n",
        "    \"\"\"\n",
        "\n",
        "    synthetic_samples = []\n",
        "    synthetic_samples_group = []\n",
        "\n",
        "    largest_group_size_label_1 = max(\n",
        "    X_train[(X_train[sensitive_attributes[0]] == val0) &\n",
        "            (X_train[sensitive_attributes[1]] == val1) &\n",
        "            (X_train[label] == 1)].shape[0]\n",
        "    for val0 in [0, 1]\n",
        "    for val1 in [0, 1]\n",
        "    )\n",
        "\n",
        "    # Determine the largest group size with X_train[label] == 0\n",
        "    largest_group_size_label_0 = max(\n",
        "        X_train[(X_train[sensitive_attributes[0]] == val0) &\n",
        "                (X_train[sensitive_attributes[1]] == val1) &\n",
        "                (X_train[label] == 0)].shape[0]\n",
        "        for val0 in [0, 1]\n",
        "        for val1 in [0, 1]\n",
        "    )\n",
        "\n",
        "    groups = sorted(group_column_train.unique())\n",
        "    #paired_groups = [(groups[i], groups[i+1]) for i in range(0, len(groups), 2)]\n",
        "\n",
        "\n",
        "    for group in groups:\n",
        "        ########## Determine pu_ix and nu_ix using reverse_group_mapping ##########\n",
        "\n",
        "        # Update the total_ratio based on the reverse_group_mapping condition\n",
        "        if reverse_group_mapping[group][2] == 1:\n",
        "            tot_ratio = largest_group_size_label_1\n",
        "        else:\n",
        "            tot_ratio = largest_group_size_label_0\n",
        "        ##########################################################################\n",
        "\n",
        "        group_df = X_train[group_column_train == group]\n",
        "        # group_df_nu = X_train[group_column_train == nu_ix]\n",
        "        # positive_count = group_df_pu[group_df_pu[label] == 1].shape[0]\n",
        "        # negative_count = group_df_nu[group_df_nu[label] == 0].shape[0]\n",
        "\n",
        "        #if positive_count == 0 or negative_count == 0:\n",
        "        #    continue\n",
        "\n",
        "        #current_ratio = positive_count / negative_count\n",
        "        if group_df.shape[0] == tot_ratio:\n",
        "            continue  # Skip the most privileged group\n",
        "\n",
        "        synthetic_points, synthetic_count = custom_smote(group_df, cat_features, group_column_train, total_ratio=tot_ratio)\n",
        "        #pu_column = np.full((len(synthetic_points), 1), os_ix)\n",
        "        synthetic_samples.append(synthetic_points)\n",
        "        #synthetic_samples_group.append(pu_column)\n",
        "        print(f\"Oversampling for group  ({group}): Added {synthetic_count} synthetic samples in {group}.\")\n",
        "\n",
        "    synthetic_samples_matrix = pd.concat(synthetic_samples, ignore_index=True)\n",
        "    #synthetic_samples_group = np.concatenate(synthetic_samples_group)\n",
        "\n",
        "    return synthetic_samples_matrix#, synthetic_samples_group\n"
      ],
      "metadata": {
        "id": "bBc69IY5xlh4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "H6wPhmpiwo7A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_optimal_epsilon(filtered_data, cat_features, min_samples, distance_matrix, eps_step=0.001, eps_min=0.01, eps_max=1.1):\n",
        "\n",
        "    def cluster_count(eps):\n",
        "        dbscan = DBSCAN(eps=eps, min_samples=min_samples, metric='precomputed')\n",
        "        labels = dbscan.fit_predict(distance_matrix)\n",
        "        unique_labels = np.unique(labels)\n",
        "        n_clusters = len(unique_labels)  # - (1 if -1 in unique_labels else 0)\n",
        "        return n_clusters, unique_labels\n",
        "\n",
        "    # Binary search for optimal epsilon\n",
        "    while eps_max - eps_min > eps_step:\n",
        "        eps_mid = (eps_min + eps_max) / 2\n",
        "        n_clusters_mid, labels_mid = cluster_count(eps_mid)\n",
        "        print(eps_mid, n_clusters_mid, labels_mid, 'mids')\n",
        "\n",
        "        if n_clusters_mid == 1:\n",
        "            if -1 in labels_mid:\n",
        "                eps_min = eps_mid  # Only noise points, increase epsilon\n",
        "            else:\n",
        "                eps_max = eps_mid  # Only core points, decrease epsilon\n",
        "        elif n_clusters_mid > 2:\n",
        "            eps_min = eps_mid  # More than two clusters, increase epsilon\n",
        "        else:\n",
        "            eps_max = eps_mid  # Exactly two clusters, continue search to fine-tune\n",
        "        print(eps_min, eps_max, 'min, max')\n",
        "    return eps_max\n",
        "\n",
        "# Custom SMOTE-DBSCAN function\n",
        "def custom_smote_dbscan(group_df, cat_features, group_column_train, total_ratio):\n",
        "    \"\"\"\n",
        "    X_train is the training dataset preprocessed, group_column_train is a column containing the group of each\n",
        "    instance in X_train\n",
        "    \"\"\"\n",
        "    #cat_attr_ix = [i for i, value in enumerate(cat_features) if value]\n",
        "\n",
        "    # X2_df = X_train[group_column_train == pu_ix]\n",
        "    # X2 = X2_df.values\n",
        "    # X3_df = X_train[group_column_train == nu_ix]\n",
        "    # X3 = X3_df.values\n",
        "\n",
        "    # PU = len(X2)\n",
        "    # NU = len(X3)\n",
        "    group_val = group_df.values\n",
        "    PU = len(group_val)\n",
        "\n",
        "    # Determine the oversampling target based on a given total_ratio\n",
        "    if PU > total_ratio:\n",
        "        print(\"it shouldn't be :(\")\n",
        "    elif PU == total_ratio:\n",
        "        print(\"The ratio of PU to NU is within the acceptable range of total_ratio.\")\n",
        "        return [], 0, #pu_ix\n",
        "    else:\n",
        "        oversampling_target = total_ratio - PU\n",
        "        os_df = group_df\n",
        "        #os_ix = pu_ix\n",
        "    os_df = os_df.reset_index(drop=True)\n",
        "\n",
        "    # Calculate min_samples\n",
        "    min_samples = round(math.log(len(os_df)))\n",
        "    distance_matrix = gower_matrix(os_df, cat_features=cat_features)\n",
        "\n",
        "    # Find the optimal epsilon for os_df\n",
        "    optimal_eps = find_optimal_epsilon(os_df, cat_features, min_samples, distance_matrix)\n",
        "\n",
        "    # DBSCAN clustering with the optimal epsilon\n",
        "    dbscan = DBSCAN(eps=optimal_eps, min_samples=min_samples, metric='precomputed')\n",
        "    clusters = dbscan.fit_predict(distance_matrix)\n",
        "\n",
        "    # Get cluster labels\n",
        "    labels = dbscan.labels_\n",
        "\n",
        "    # Get core samples\n",
        "    core_samples_mask = np.zeros_like(labels, dtype=bool)\n",
        "    core_samples_mask[dbscan.core_sample_indices_] = True\n",
        "\n",
        "    # Identify core, border, and noise points\n",
        "    core_points = os_df[core_samples_mask]\n",
        "    border_points = os_df[~core_samples_mask & (labels != -1)]\n",
        "    noise_points = os_df[labels == -1]\n",
        "\n",
        "    if len(border_points) == 0:\n",
        "        border_points = core_points\n",
        "\n",
        "    # Initialize synthetic samples list\n",
        "    synthetic_samples = []\n",
        "\n",
        "    border_indices = border_points.index.tolist()\n",
        "    random.shuffle(border_indices)\n",
        "    current_index = 0\n",
        "\n",
        "    while len(synthetic_samples) < oversampling_target:\n",
        "        idx_A = border_indices[current_index % len(border_indices)]\n",
        "        current_index += 1\n",
        "        point_A = os_df.loc[idx_A]\n",
        "\n",
        "        # Ensure point B is not a noise point\n",
        "        distances_to_A = distance_matrix[idx_A]\n",
        "        neighbors = np.argsort(distances_to_A)[1:min_samples+1]  # Exclude the point itself\n",
        "        valid_neighbors = [idx for idx in neighbors if labels[idx] != -1]  # Exclude noise points\n",
        "\n",
        "        if not valid_neighbors:\n",
        "            continue  # Skip if no valid neighbors are found\n",
        "\n",
        "        idx_B = np.random.choice(valid_neighbors)\n",
        "        point_B = os_df.loc[idx_B]\n",
        "\n",
        "        synthetic_point = {}\n",
        "        for i, col in enumerate(os_df.columns):\n",
        "            if cat_features[i]:\n",
        "                neighbor_values = os_df.iloc[valid_neighbors][col].tolist()\n",
        "                synthetic_point[col] = max(set(neighbor_values), key=neighbor_values.count)\n",
        "            else:\n",
        "                alpha = np.random.rand()\n",
        "                synthetic_point[col] = point_A[col] + alpha * (point_B[col] - point_A[col])\n",
        "\n",
        "        synthetic_samples.append(synthetic_point)\n",
        "\n",
        "    return pd.DataFrame(synthetic_samples), len(synthetic_samples)\n",
        "\n",
        "# Example usage (assuming you have defined X_train, cat_features, etc.):\n",
        "# synthetic_samples, num_samples, oversampled_index = custom_smote_dbscan(X_train, cat_features, pu_ix, nu_ix, group_column_train, total_ratio)\n"
      ],
      "metadata": {
        "id": "OZ4xVr3gwsgQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "synthetic_samples_matrix_dbscan = oversample_groups(X_train, cat_features, custom_smote_dbscan, subgroup_column_train, sensitive_attributes, label, reverse_group_mapping)\n",
        "\n",
        "# Concatenate the original dataset with the synthetic samples\n",
        "X_train_resampled_dbscan = pd.concat([X_train, pd.DataFrame(synthetic_samples_matrix_dbscan, columns=X_train.columns)], ignore_index=True)\n",
        "#subgroup_column_resampled_tax = pd.concat([X_train[group_column_train], pd.Series(synthetic_samples_group_tax.flatten())], ignore_index=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9MKFIoUcnDtE",
        "outputId": "d4fe0e7a-7ff2-4897-d4dc-435f6e80bad6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.555 1 [0] mids\n",
            "0.01 0.555 min, max\n",
            "0.28250000000000003 1 [0] mids\n",
            "0.01 0.28250000000000003 min, max\n",
            "0.14625000000000002 3 [-1  0  1] mids\n",
            "0.14625000000000002 0.28250000000000003 min, max\n",
            "0.21437500000000004 2 [-1  0] mids\n",
            "0.14625000000000002 0.21437500000000004 min, max\n",
            "0.18031250000000004 2 [-1  0] mids\n",
            "0.14625000000000002 0.18031250000000004 min, max\n",
            "0.16328125000000004 2 [-1  0] mids\n",
            "0.14625000000000002 0.16328125000000004 min, max\n",
            "0.15476562500000002 2 [-1  0] mids\n",
            "0.14625000000000002 0.15476562500000002 min, max\n",
            "0.1505078125 3 [-1  0  1] mids\n",
            "0.1505078125 0.15476562500000002 min, max\n",
            "0.15263671875 3 [-1  0  1] mids\n",
            "0.15263671875 0.15476562500000002 min, max\n",
            "0.15370117187500001 3 [-1  0  1] mids\n",
            "0.15370117187500001 0.15476562500000002 min, max\n",
            "0.1542333984375 2 [-1  0] mids\n",
            "0.15370117187500001 0.1542333984375 min, max\n",
            "Oversampling for group  (2): Added 5488 synthetic samples in 2.\n",
            "0.555 1 [0] mids\n",
            "0.01 0.555 min, max\n",
            "0.28250000000000003 1 [0] mids\n",
            "0.01 0.28250000000000003 min, max\n",
            "0.14625000000000002 3 [-1  0  1] mids\n",
            "0.14625000000000002 0.28250000000000003 min, max\n",
            "0.21437500000000004 2 [-1  0] mids\n",
            "0.14625000000000002 0.21437500000000004 min, max\n",
            "0.18031250000000004 2 [-1  0] mids\n",
            "0.14625000000000002 0.18031250000000004 min, max\n",
            "0.16328125000000004 2 [-1  0] mids\n",
            "0.14625000000000002 0.16328125000000004 min, max\n",
            "0.15476562500000002 2 [-1  0] mids\n",
            "0.14625000000000002 0.15476562500000002 min, max\n",
            "0.1505078125 3 [-1  0  1] mids\n",
            "0.1505078125 0.15476562500000002 min, max\n",
            "0.15263671875 3 [-1  0  1] mids\n",
            "0.15263671875 0.15476562500000002 min, max\n",
            "0.15370117187500001 3 [-1  0  1] mids\n",
            "0.15370117187500001 0.15476562500000002 min, max\n",
            "0.1542333984375 2 [-1  0] mids\n",
            "0.15370117187500001 0.1542333984375 min, max\n",
            "Oversampling for group  (3): Added 5107 synthetic samples in 3.\n",
            "0.555 1 [0] mids\n",
            "0.01 0.555 min, max\n",
            "0.28250000000000003 1 [0] mids\n",
            "0.01 0.28250000000000003 min, max\n",
            "0.14625000000000002 2 [-1  0] mids\n",
            "0.01 0.14625000000000002 min, max\n",
            "0.07812500000000001 4 [-1  0  1  2] mids\n",
            "0.07812500000000001 0.14625000000000002 min, max\n",
            "0.11218750000000002 2 [-1  0] mids\n",
            "0.07812500000000001 0.11218750000000002 min, max\n",
            "0.09515625000000003 2 [-1  0] mids\n",
            "0.07812500000000001 0.09515625000000003 min, max\n",
            "0.08664062500000003 2 [-1  0] mids\n",
            "0.07812500000000001 0.08664062500000003 min, max\n",
            "0.08238281250000001 2 [-1  0] mids\n",
            "0.07812500000000001 0.08238281250000001 min, max\n",
            "0.08025390625 3 [-1  0  1] mids\n",
            "0.08025390625 0.08238281250000001 min, max\n",
            "0.08131835937500001 4 [-1  0  1  2] mids\n",
            "0.08131835937500001 0.08238281250000001 min, max\n",
            "0.08185058593750001 4 [-1  0  1  2] mids\n",
            "0.08185058593750001 0.08238281250000001 min, max\n",
            "Oversampling for group  (4): Added 11572 synthetic samples in 4.\n",
            "0.555 1 [0] mids\n",
            "0.01 0.555 min, max\n",
            "0.28250000000000003 1 [0] mids\n",
            "0.01 0.28250000000000003 min, max\n",
            "0.14625000000000002 3 [-1  0  1] mids\n",
            "0.14625000000000002 0.28250000000000003 min, max\n",
            "0.21437500000000004 2 [-1  0] mids\n",
            "0.14625000000000002 0.21437500000000004 min, max\n",
            "0.18031250000000004 2 [-1  0] mids\n",
            "0.14625000000000002 0.18031250000000004 min, max\n",
            "0.16328125000000004 2 [-1  0] mids\n",
            "0.14625000000000002 0.16328125000000004 min, max\n",
            "0.15476562500000002 2 [-1  0] mids\n",
            "0.14625000000000002 0.15476562500000002 min, max\n",
            "0.1505078125 3 [-1  0  1] mids\n",
            "0.1505078125 0.15476562500000002 min, max\n",
            "0.15263671875 3 [-1  0  1] mids\n",
            "0.15263671875 0.15476562500000002 min, max\n",
            "0.15370117187500001 3 [-1  0  1] mids\n",
            "0.15370117187500001 0.15476562500000002 min, max\n",
            "0.1542333984375 2 [-1  0] mids\n",
            "0.15370117187500001 0.1542333984375 min, max\n",
            "Oversampling for group  (5): Added 5840 synthetic samples in 5.\n",
            "0.555 1 [0] mids\n",
            "0.01 0.555 min, max\n",
            "0.28250000000000003 1 [0] mids\n",
            "0.01 0.28250000000000003 min, max\n",
            "0.14625000000000002 2 [-1  0] mids\n",
            "0.01 0.14625000000000002 min, max\n",
            "0.07812500000000001 5 [-1  0  1  2  3] mids\n",
            "0.07812500000000001 0.14625000000000002 min, max\n",
            "0.11218750000000002 2 [-1  0] mids\n",
            "0.07812500000000001 0.11218750000000002 min, max\n",
            "0.09515625000000003 2 [-1  0] mids\n",
            "0.07812500000000001 0.09515625000000003 min, max\n",
            "0.08664062500000003 2 [-1  0] mids\n",
            "0.07812500000000001 0.08664062500000003 min, max\n",
            "0.08238281250000001 2 [-1  0] mids\n",
            "0.07812500000000001 0.08238281250000001 min, max\n",
            "0.08025390625 2 [-1  0] mids\n",
            "0.07812500000000001 0.08025390625 min, max\n",
            "0.079189453125 3 [-1  0  1] mids\n",
            "0.079189453125 0.08025390625 min, max\n",
            "0.0797216796875 3 [-1  0  1] mids\n",
            "0.0797216796875 0.08025390625 min, max\n",
            "Oversampling for group  (6): Added 11416 synthetic samples in 6.\n",
            "0.555 1 [0] mids\n",
            "0.01 0.555 min, max\n",
            "0.28250000000000003 2 [-1  0] mids\n",
            "0.01 0.28250000000000003 min, max\n",
            "0.14625000000000002 3 [-1  0  1] mids\n",
            "0.14625000000000002 0.28250000000000003 min, max\n",
            "0.21437500000000004 2 [-1  0] mids\n",
            "0.14625000000000002 0.21437500000000004 min, max\n",
            "0.18031250000000004 2 [-1  0] mids\n",
            "0.14625000000000002 0.18031250000000004 min, max\n",
            "0.16328125000000004 2 [-1  0] mids\n",
            "0.14625000000000002 0.16328125000000004 min, max\n",
            "0.15476562500000002 3 [-1  0  1] mids\n",
            "0.15476562500000002 0.16328125000000004 min, max\n",
            "0.15902343750000003 2 [-1  0] mids\n",
            "0.15476562500000002 0.15902343750000003 min, max\n",
            "0.15689453125000002 2 [-1  0] mids\n",
            "0.15476562500000002 0.15689453125000002 min, max\n",
            "0.15583007812500002 3 [-1  0  1] mids\n",
            "0.15583007812500002 0.15689453125000002 min, max\n",
            "0.1563623046875 2 [-1  0] mids\n",
            "0.15583007812500002 0.1563623046875 min, max\n",
            "Oversampling for group  (7): Added 6038 synthetic samples in 7.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "WCjeT5_JMfLP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#################################################################################\n",
        "num_privileged_ones = X_train[(X_train[sensitive_attributes[0]] == 1) &\n",
        "                              (X_train[sensitive_attributes[1]] == 1) &\n",
        "                              (X_train[label] == 1)].shape[0]\n",
        "\n",
        "num_privileged_zeros = X_train[(X_train[sensitive_attributes[0]]) &\n",
        "                               (X_train[sensitive_attributes[1]] == 1) &\n",
        "                               (X_train[label] == 0)].shape[0]\n",
        "\n",
        "# Calculating the ratio of the most privileged class\n",
        "total_ratio = num_privileged_ones / num_privileged_zeros if num_privileged_zeros != 0 else float('inf')  # Avoid division by zero\n",
        "\n",
        "print(f\"Ratio of most privileged class: {total_ratio}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i5caQzQkMtkC",
        "outputId": "7672c9cf-6dff-4d8c-e501-67053458c285"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ratio of most privileged class: 0.4790803159458825\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def find_optimal_epsilon_2(filtered_data, cat_features, min_samples, distance_matrix, eps_step=0.001, eps_min=0.01, eps_max=1.1):\n",
        "\n",
        "    def cluster_count(eps):\n",
        "        dbscan = DBSCAN(eps=eps, min_samples=min_samples, metric='precomputed')\n",
        "        labels = dbscan.fit_predict(distance_matrix)\n",
        "        unique_labels = np.unique(labels)\n",
        "        n_clusters = len(unique_labels)  # - (1 if -1 in unique_labels else 0)\n",
        "        return n_clusters, unique_labels\n",
        "\n",
        "    # Binary search for optimal epsilon\n",
        "    while eps_max - eps_min > eps_step:\n",
        "        eps_mid = (eps_min + eps_max) / 2\n",
        "        n_clusters_mid, labels_mid = cluster_count(eps_mid)\n",
        "        print(eps_mid, n_clusters_mid, labels_mid, 'mids')\n",
        "\n",
        "        if n_clusters_mid == 1:\n",
        "            if -1 in labels_mid:\n",
        "                eps_min = eps_mid  # Only noise points, increase epsilon\n",
        "            else:\n",
        "                eps_max = eps_mid  # Only core points, decrease epsilon\n",
        "        elif n_clusters_mid > 2:\n",
        "            eps_min = eps_mid  # More than two clusters, increase epsilon\n",
        "        else:\n",
        "            eps_max = eps_mid  # Exactly two clusters, continue search to fine-tune\n",
        "        print(eps_min, eps_max, 'min, max')\n",
        "    return eps_max\n",
        "\n",
        "# Custom SMOTE-DBSCAN function\n",
        "def custom_smote_dbscan_ratio(X_train, cat_features, pu_ix, nu_ix, group_column_train, total_ratio):\n",
        "    \"\"\"\n",
        "    X_train is the training dataset preprocessed, group_column_train is a column containing the group of each\n",
        "    instance in X_train\n",
        "    \"\"\"\n",
        "    cat_attr_ix = [i for i, value in enumerate(cat_features) if value]\n",
        "\n",
        "    X2_df = X_train[group_column_train == pu_ix]\n",
        "    X2 = X2_df.values\n",
        "    X3_df = X_train[group_column_train == nu_ix]\n",
        "    X3 = X3_df.values\n",
        "\n",
        "    PU = len(X2)\n",
        "    NU = len(X3)\n",
        "\n",
        "    # Determine the oversampling target based on a given total_ratio\n",
        "    if (PU / NU) > total_ratio:\n",
        "        oversampling_target = (PU / total_ratio) - NU\n",
        "        os_df = X3_df\n",
        "        os_ix = nu_ix\n",
        "    elif (PU / NU) == total_ratio:\n",
        "        print(\"The ratio of PU to NU is within the acceptable range of total_ratio.\")\n",
        "        return [], 0, pu_ix\n",
        "    else:\n",
        "        oversampling_target = (total_ratio * NU) - PU\n",
        "        os_df = X2_df\n",
        "        os_ix = pu_ix\n",
        "    os_df = os_df.reset_index(drop=True)\n",
        "\n",
        "    # Calculate min_samples\n",
        "    min_samples = round(math.log(len(os_df)))\n",
        "    distance_matrix = gower_matrix(os_df, cat_features=cat_features)\n",
        "\n",
        "    # Find the optimal epsilon for os_df\n",
        "    optimal_eps = find_optimal_epsilon_2(os_df, cat_features, min_samples, distance_matrix)\n",
        "\n",
        "    # DBSCAN clustering with the optimal epsilon\n",
        "    dbscan = DBSCAN(eps=optimal_eps, min_samples=min_samples, metric='precomputed')\n",
        "    clusters = dbscan.fit_predict(distance_matrix)\n",
        "\n",
        "    # Get cluster labels\n",
        "    labels = dbscan.labels_\n",
        "\n",
        "    # Get core samples\n",
        "    core_samples_mask = np.zeros_like(labels, dtype=bool)\n",
        "    core_samples_mask[dbscan.core_sample_indices_] = True\n",
        "\n",
        "    # Identify core, border, and noise points\n",
        "    core_points = os_df[core_samples_mask]\n",
        "    border_points = os_df[~core_samples_mask & (labels != -1)]\n",
        "    noise_points = os_df[labels == -1]\n",
        "\n",
        "    if len(border_points) == 0:\n",
        "        border_points = core_points\n",
        "\n",
        "    # Initialize synthetic samples list\n",
        "    synthetic_samples = []\n",
        "\n",
        "    border_indices = border_points.index.tolist()\n",
        "    random.shuffle(border_indices)\n",
        "    current_index = 0\n",
        "\n",
        "    while len(synthetic_samples) < oversampling_target:\n",
        "        idx_A = border_indices[current_index % len(border_indices)]\n",
        "        current_index += 1\n",
        "        point_A = os_df.loc[idx_A]\n",
        "\n",
        "        # Ensure point B is not a noise point\n",
        "        distances_to_A = distance_matrix[idx_A]\n",
        "        neighbors = np.argsort(distances_to_A)[1:min_samples+1]  # Exclude the point itself\n",
        "        valid_neighbors = [idx for idx in neighbors if labels[idx] != -1]  # Exclude noise points\n",
        "\n",
        "        if not valid_neighbors:\n",
        "            continue  # Skip if no valid neighbors are found\n",
        "\n",
        "        idx_B = np.random.choice(valid_neighbors)\n",
        "        point_B = os_df.loc[idx_B]\n",
        "\n",
        "        synthetic_point = {}\n",
        "        for i, col in enumerate(os_df.columns):\n",
        "            if cat_features[i]:\n",
        "                neighbor_values = os_df.iloc[valid_neighbors][col].tolist()\n",
        "                synthetic_point[col] = max(set(neighbor_values), key=neighbor_values.count)\n",
        "            else:\n",
        "                alpha = np.random.rand()\n",
        "                synthetic_point[col] = point_A[col] + alpha * (point_B[col] - point_A[col])\n",
        "\n",
        "        synthetic_samples.append(synthetic_point)\n",
        "\n",
        "    return pd.DataFrame(synthetic_samples), len(synthetic_samples), os_ix\n",
        "\n",
        "# Example usage (assuming you have defined X_train, cat_features, etc.):\n",
        "# synthetic_samples, num_samples, oversampled_index = custom_smote_dbscan(X_train, cat_features, pu_ix, nu_ix, group_column_train, total_ratio)\n"
      ],
      "metadata": {
        "id": "3UcX7zxHMf1z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def oversample_groups_ratio(X_train, cat_features, custom_smote, group_column_train, total_ratio, reverse_group_mapping):\n",
        "    \"\"\"\n",
        "    Function to oversample multiple groups automatically based on group labels.\n",
        "\n",
        "    Parameters:\n",
        "    - X_train: Preprocessed training dataset.\n",
        "    - cat_features: List indicating categorical features.\n",
        "    - custom_smote: Custom SMOTE function to be used.\n",
        "    - group_column_train: Column containing the group label for each instance.\n",
        "    - total_ratio: Desired ratio of positive to negative labels.\n",
        "    - reverse_group_mapping: Mapping of groups to sensitive attributes and labels.\n",
        "\n",
        "    Returns:\n",
        "    - synthetic_samples_matrix: Matrix containing all generated synthetic samples.\n",
        "    - synthetic_samples_group: Array of group labels for the synthetic samples.\n",
        "    \"\"\"\n",
        "\n",
        "    synthetic_samples = []\n",
        "    synthetic_samples_group = []\n",
        "\n",
        "    groups = sorted(group_column_train.unique())\n",
        "    paired_groups = [(groups[i], groups[i+1]) for i in range(0, len(groups), 2)]\n",
        "\n",
        "    for group1, group2 in paired_groups:\n",
        "        ########## Determine pu_ix and nu_ix using reverse_group_mapping ##########\n",
        "        if reverse_group_mapping[group1][2] == 1:\n",
        "            pu_ix = group1\n",
        "            nu_ix = group2\n",
        "        else:\n",
        "            pu_ix = group2\n",
        "            nu_ix = group1\n",
        "        ##########################################################################\n",
        "\n",
        "        group_df_pu = X_train[group_column_train == pu_ix]\n",
        "        group_df_nu = X_train[group_column_train == nu_ix]\n",
        "        positive_count = group_df_pu[group_df_pu[label] == 1].shape[0]\n",
        "        negative_count = group_df_nu[group_df_nu[label] == 0].shape[0]\n",
        "\n",
        "        if positive_count == 0 or negative_count == 0:\n",
        "            continue\n",
        "\n",
        "        current_ratio = positive_count / negative_count\n",
        "\n",
        "        if current_ratio == total_ratio:\n",
        "            continue  # Skip the most privileged group\n",
        "\n",
        "        synthetic_points, synthetic_count, os_ix = custom_smote(X_train, cat_features, pu_ix, nu_ix, group_column_train, total_ratio=total_ratio)\n",
        "        pu_column = np.full((len(synthetic_points), 1), os_ix)\n",
        "        synthetic_samples.append(synthetic_points)\n",
        "        synthetic_samples_group.append(pu_column)\n",
        "        print(f\"Oversampling for group pair ({pu_ix}, {nu_ix}): Added {synthetic_count} synthetic samples in {os_ix}.\")\n",
        "\n",
        "    synthetic_samples_matrix = pd.concat(synthetic_samples, ignore_index=True)\n",
        "    synthetic_samples_group = np.concatenate(synthetic_samples_group)\n",
        "\n",
        "    return synthetic_samples_matrix, synthetic_samples_group\n"
      ],
      "metadata": {
        "id": "jgQ0ytdbM5Xl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "synthetic_samples_matrix_dbscan_ratio, synthetic_samples_group_dbscan_ratio = oversample_groups_ratio(X_train, cat_features, custom_smote_dbscan_ratio, subgroup_column_train, total_ratio, reverse_group_mapping)\n",
        "\n",
        "# Concatenate the original dataset with the synthetic samples\n",
        "X_train_resampled_dbscan_ratio = pd.concat([X_train, pd.DataFrame(synthetic_samples_matrix_dbscan_ratio, columns=X_train.columns)], ignore_index=True)\n",
        "#subgroup_column_resampled_tax = pd.concat([X_train[group_column_train], pd.Series(synthetic_samples_group_tax.flatten())], ignore_index=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oj2QxeOGM9jx",
        "outputId": "78250d31-7305-482f-9187-2307dc2e2a2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.555 1 [0] mids\n",
            "0.01 0.555 min, max\n",
            "0.28250000000000003 1 [0] mids\n",
            "0.01 0.28250000000000003 min, max\n",
            "0.14625000000000002 3 [-1  0  1] mids\n",
            "0.14625000000000002 0.28250000000000003 min, max\n",
            "0.21437500000000004 2 [-1  0] mids\n",
            "0.14625000000000002 0.21437500000000004 min, max\n",
            "0.18031250000000004 2 [-1  0] mids\n",
            "0.14625000000000002 0.18031250000000004 min, max\n",
            "0.16328125000000004 2 [-1  0] mids\n",
            "0.14625000000000002 0.16328125000000004 min, max\n",
            "0.15476562500000002 2 [-1  0] mids\n",
            "0.14625000000000002 0.15476562500000002 min, max\n",
            "0.1505078125 3 [-1  0  1] mids\n",
            "0.1505078125 0.15476562500000002 min, max\n",
            "0.15263671875 3 [-1  0  1] mids\n",
            "0.15263671875 0.15476562500000002 min, max\n",
            "0.15370117187500001 3 [-1  0  1] mids\n",
            "0.15370117187500001 0.15476562500000002 min, max\n",
            "0.1542333984375 2 [-1  0] mids\n",
            "0.15370117187500001 0.1542333984375 min, max\n",
            "Oversampling for group pair (3, 2): Added 2478 synthetic samples in 3.\n",
            "0.555 1 [0] mids\n",
            "0.01 0.555 min, max\n",
            "0.28250000000000003 1 [0] mids\n",
            "0.01 0.28250000000000003 min, max\n",
            "0.14625000000000002 3 [-1  0  1] mids\n",
            "0.14625000000000002 0.28250000000000003 min, max\n",
            "0.21437500000000004 2 [-1  0] mids\n",
            "0.14625000000000002 0.21437500000000004 min, max\n",
            "0.18031250000000004 2 [-1  0] mids\n",
            "0.14625000000000002 0.18031250000000004 min, max\n",
            "0.16328125000000004 2 [-1  0] mids\n",
            "0.14625000000000002 0.16328125000000004 min, max\n",
            "0.15476562500000002 2 [-1  0] mids\n",
            "0.14625000000000002 0.15476562500000002 min, max\n",
            "0.1505078125 3 [-1  0  1] mids\n",
            "0.1505078125 0.15476562500000002 min, max\n",
            "0.15263671875 3 [-1  0  1] mids\n",
            "0.15263671875 0.15476562500000002 min, max\n",
            "0.15370117187500001 3 [-1  0  1] mids\n",
            "0.15370117187500001 0.15476562500000002 min, max\n",
            "0.1542333984375 2 [-1  0] mids\n",
            "0.15370117187500001 0.1542333984375 min, max\n",
            "Oversampling for group pair (5, 4): Added 297 synthetic samples in 5.\n",
            "0.555 1 [0] mids\n",
            "0.01 0.555 min, max\n",
            "0.28250000000000003 2 [-1  0] mids\n",
            "0.01 0.28250000000000003 min, max\n",
            "0.14625000000000002 3 [-1  0  1] mids\n",
            "0.14625000000000002 0.28250000000000003 min, max\n",
            "0.21437500000000004 2 [-1  0] mids\n",
            "0.14625000000000002 0.21437500000000004 min, max\n",
            "0.18031250000000004 2 [-1  0] mids\n",
            "0.14625000000000002 0.18031250000000004 min, max\n",
            "0.16328125000000004 2 [-1  0] mids\n",
            "0.14625000000000002 0.16328125000000004 min, max\n",
            "0.15476562500000002 3 [-1  0  1] mids\n",
            "0.15476562500000002 0.16328125000000004 min, max\n",
            "0.15902343750000003 2 [-1  0] mids\n",
            "0.15476562500000002 0.15902343750000003 min, max\n",
            "0.15689453125000002 2 [-1  0] mids\n",
            "0.15476562500000002 0.15689453125000002 min, max\n",
            "0.15583007812500002 3 [-1  0  1] mids\n",
            "0.15583007812500002 0.15689453125000002 min, max\n",
            "0.1563623046875 2 [-1  0] mids\n",
            "0.15583007812500002 0.1563623046875 min, max\n",
            "Oversampling for group pair (7, 6): Added 569 synthetic samples in 7.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "CLASSIFICATION\n"
      ],
      "metadata": {
        "id": "YLZqOzodOh4o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model_performance(X_train, X_test, protected_attributes, label_name, groups, model, weights=None):\n",
        "    favorable_label = 1.0\n",
        "    unfavorable_label = 0.0\n",
        "    X_train[label_name] = X_train[label_name].astype(float)\n",
        "    X_test[label_name] = X_test[label_name].astype(float)\n",
        "    # If weights is not provided, create an array of ones with the same length as X_train\n",
        "    if weights is None:\n",
        "        weights = np.ones(len(X_train))\n",
        "\n",
        "    # Create BinaryLabelDatasets\n",
        "    binary_ds_train = BinaryLabelDataset(df=X_train, label_names=[label_name],\n",
        "                                         protected_attribute_names=protected_attributes,\n",
        "                                         favorable_label=favorable_label, unfavorable_label=unfavorable_label)\n",
        "    binary_ds_test = BinaryLabelDataset(df=X_test, label_names=[label_name],\n",
        "                                        protected_attribute_names=protected_attributes,\n",
        "                                        favorable_label=favorable_label, unfavorable_label=unfavorable_label)\n",
        "    if model == 'Logistic Regression':\n",
        "        classifier = LogisticRegression(max_iter = 300)\n",
        "    elif model == 'Random Forest':\n",
        "        classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "    elif model == 'Gradient Boosting':\n",
        "        classifier = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, random_state=42)\n",
        "    else:\n",
        "        raise ValueError('Choose one classification algorithm between Logistic Regression, Random Forest, Gradient Boosting')\n",
        "\n",
        "    classifier.fit(X_train.drop(columns=[label_name]), X_train[label_name], sample_weight=weights)\n",
        "    predicted_labels = classifier.predict(X_test.drop(columns=[label_name]))\n",
        "\n",
        "    X_test_with_predictions = pd.concat([X_test.drop(columns=[label_name]), pd.Series(predicted_labels, name=label_name, index=X_test.index)], axis=1)\n",
        "\n",
        "    binary_ds_test_pred = BinaryLabelDataset(df=X_test_with_predictions, label_names=[label_name],\n",
        "                                             protected_attribute_names=protected_attributes,\n",
        "                                             favorable_label=favorable_label, unfavorable_label=unfavorable_label)\n",
        "\n",
        "    all_results = {}\n",
        "    for (group1, group2) in itertools.combinations(groups, 2):\n",
        "        print(group1, group2, 'gruppi')\n",
        "        pair_key = f\"{group1['name']} vs {group2['name']}\"\n",
        "        all_results[pair_key] = evaluate(\n",
        "            binary_ds_test, binary_ds_test_pred,\n",
        "            [group1['attributes']], [group2['attributes']])\n",
        "        #print([group1['attributes']], [group2['attributes']])\n",
        "\n",
        "\n",
        "    return all_results, predicted_labels"
      ],
      "metadata": {
        "id": "Df4gOIg4Okjo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(test_data, pred, priv_group, unpriv_group):\n",
        "    cm = ClassificationMetric(test_data, pred,\n",
        "                              unprivileged_groups=unpriv_group,\n",
        "                              privileged_groups=priv_group)\n",
        "    dm = BinaryLabelDatasetMetric(pred,\n",
        "                                  unprivileged_groups=unpriv_group,\n",
        "                                  privileged_groups=priv_group)\n",
        "\n",
        "    measure_scores = {\n",
        "        'Balanced Accuracy': balanced_accuracy_score(test_data.labels, pred.labels),\n",
        "        'Accuracy': cm.accuracy(),\n",
        "        'F1 Score': f1_score(test_data.labels.ravel(), pred.labels.ravel()),  # Ensure labels are flat\n",
        "        'Disparate Impact Ratio': dm.disparate_impact(),\n",
        "        #'Demographic Parity Difference': cm.statistical_parity_difference(),\n",
        "        #'Predictive Parity Difference': cm.positive_predictive_value(privileged=True) - cm.positive_predictive_value(privileged=False),\n",
        "        'Average Odds Difference': cm.average_odds_difference(),\n",
        "        'Equal Opportunity Difference': cm.equal_opportunity_difference(),\n",
        "        #'Equalized Odds Difference': cm.average_abs_odds_difference(),\n",
        "        'Consistency': dm.consistency(),\n",
        "        #'TPR Difference': cm.true_positive_rate_difference(),\n",
        "        #'FPR Difference': cm.false_positive_rate_difference(),\n",
        "        #'TNR Difference': cm.true_negative_rate(privileged=True) - cm.true_negative_rate(privileged=False),\n",
        "        #'FNR Difference': cm.false_negative_rate_difference(),\n",
        "    }\n",
        "\n",
        "    return measure_scores"
      ],
      "metadata": {
        "id": "x9iNhj2cO8hX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(df, actual_labels, predicted_labels):\n",
        "    \"\"\"Compute fairness and performance metrics.\"\"\"\n",
        "    cm = confusion_matrix(actual_labels, predicted_labels)\n",
        "    TN, FP, FN, TP = cm.ravel()\n",
        "    metrics = {\n",
        "        'Accuracy': accuracy_score(actual_labels, predicted_labels),\n",
        "        'Precision': precision_score(actual_labels, predicted_labels),\n",
        "        'Recall': recall_score(actual_labels, predicted_labels),\n",
        "        'F1 Score': f1_score(actual_labels, predicted_labels),\n",
        "        'TPR': TP / (TP + FN),\n",
        "        'FPR': FP / (FP + TN),\n",
        "        'TNR': TN / (TN + FP),\n",
        "        'FNR': FN / (FN + TP),\n",
        "        'TP': TP,\n",
        "        'FP': FP,\n",
        "        'TN': TN,\n",
        "        'FN': FN\n",
        "    }\n",
        "\n",
        "    return metrics"
      ],
      "metadata": {
        "id": "4scy_yg3O_ZU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##################################################################################\n",
        "results_orig, pred_labels_orig = evaluate_model_performance(X_train, X_test, sensitive_attributes, label,\n",
        "                                                            groups, model=model)\n",
        "#model = 'Random Forest'\n",
        "#model = 'Gradient Boosting'\n",
        "\n",
        "# Initialize a list to hold DataFrames\n",
        "data_frames = []\n",
        "\n",
        "# Populate the list with DataFrames, each having a unique row index\n",
        "for key, values in results_orig.items():\n",
        "    df_part = pd.DataFrame([values], index=[key])\n",
        "    data_frames.append(df_part)\n",
        "\n",
        "# Concatenate all DataFrames into a single DataFrame\n",
        "results_orig_df = pd.concat(data_frames)\n",
        "results_orig_df.index.name = 'Comparison'\n",
        "\n",
        "# Print the results DataFrame\n",
        "print(results_orig_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zEUuxk4jPGh4",
        "outputId": "45751ed3-992f-4537-a2af-b723ee3d73af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'name': 'White Male', 'attributes': {'race': 1, 'sex': 1}} {'name': 'Black Male', 'attributes': {'race': 0, 'sex': 1}} gruppi\n",
            "{'name': 'White Male', 'attributes': {'race': 1, 'sex': 1}} {'name': 'White Female', 'attributes': {'race': 1, 'sex': 0}} gruppi\n",
            "{'name': 'White Male', 'attributes': {'race': 1, 'sex': 1}} {'name': 'Black Female', 'attributes': {'race': 0, 'sex': 0}} gruppi\n",
            "{'name': 'Black Male', 'attributes': {'race': 0, 'sex': 1}} {'name': 'White Female', 'attributes': {'race': 1, 'sex': 0}} gruppi\n",
            "{'name': 'Black Male', 'attributes': {'race': 0, 'sex': 1}} {'name': 'Black Female', 'attributes': {'race': 0, 'sex': 0}} gruppi\n",
            "{'name': 'White Female', 'attributes': {'race': 1, 'sex': 0}} {'name': 'Black Female', 'attributes': {'race': 0, 'sex': 0}} gruppi\n",
            "                              Balanced Accuracy  Accuracy  F1 Score  \\\n",
            "Comparison                                                            \n",
            "White Male vs Black Male               0.637464   0.79575  0.439924   \n",
            "White Male vs White Female             0.637464   0.79575  0.439924   \n",
            "White Male vs Black Female             0.637464   0.79575  0.439924   \n",
            "Black Male vs White Female             0.637464   0.79575  0.439924   \n",
            "Black Male vs Black Female             0.637464   0.79575  0.439924   \n",
            "White Female vs Black Female           0.637464   0.79575  0.439924   \n",
            "\n",
            "                              Disparate Impact Ratio  Average Odds Difference  \\\n",
            "Comparison                                                                      \n",
            "White Male vs Black Male                    0.359960                -0.094888   \n",
            "White Male vs White Female                  0.205024                -0.110879   \n",
            "White Male vs Black Female                  0.077964                -0.144617   \n",
            "Black Male vs White Female                  0.569573                -0.015991   \n",
            "Black Male vs Black Female                  0.216589                -0.049729   \n",
            "White Female vs Black Female                0.380267                -0.033738   \n",
            "\n",
            "                              Equal Opportunity Difference  \\\n",
            "Comparison                                                   \n",
            "White Male vs Black Male                         -0.138370   \n",
            "White Male vs White Female                       -0.158825   \n",
            "White Male vs Black Female                       -0.219906   \n",
            "Black Male vs White Female                       -0.020454   \n",
            "Black Male vs Black Female                       -0.081536   \n",
            "White Female vs Black Female                     -0.061082   \n",
            "\n",
            "                                       Consistency  \n",
            "Comparison                                          \n",
            "White Male vs Black Male      [0.9529366306027821]  \n",
            "White Male vs White Female    [0.9529366306027821]  \n",
            "White Male vs Black Female    [0.9529366306027821]  \n",
            "Black Male vs White Female    [0.9529366306027821]  \n",
            "Black Male vs Black Female    [0.9529366306027821]  \n",
            "White Female vs Black Female  [0.9529366306027821]  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##################################################################################\n",
        "results_dbscan, pred_labels_dbscan = evaluate_model_performance(X_train_resampled_dbscan, X_test, sensitive_attributes, label,\n",
        "                                                            groups, model=model)\n",
        "\n",
        "# Initialize a list to hold DataFrames\n",
        "data_frames = []\n",
        "\n",
        "# Populate the list with DataFrames, each having a unique row index\n",
        "for key, values in results_dbscan.items():\n",
        "    df_part = pd.DataFrame([values], index=[key])\n",
        "    data_frames.append(df_part)\n",
        "\n",
        "# Concatenate all DataFrames into a single DataFrame\n",
        "results_dbscan_df = pd.concat(data_frames)\n",
        "results_dbscan_df.index.name = 'Comparison'\n",
        "\n",
        "# Print the results DataFrame\n",
        "print(results_dbscan_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3JhafVEPPOR3",
        "outputId": "1535db67-16d8-4f0d-d6c2-fc66b8ceee0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'name': 'White Male', 'attributes': {'race': 1, 'sex': 1}} {'name': 'Black Male', 'attributes': {'race': 0, 'sex': 1}} gruppi\n",
            "{'name': 'White Male', 'attributes': {'race': 1, 'sex': 1}} {'name': 'White Female', 'attributes': {'race': 1, 'sex': 0}} gruppi\n",
            "{'name': 'White Male', 'attributes': {'race': 1, 'sex': 1}} {'name': 'Black Female', 'attributes': {'race': 0, 'sex': 0}} gruppi\n",
            "{'name': 'Black Male', 'attributes': {'race': 0, 'sex': 1}} {'name': 'White Female', 'attributes': {'race': 1, 'sex': 0}} gruppi\n",
            "{'name': 'Black Male', 'attributes': {'race': 0, 'sex': 1}} {'name': 'Black Female', 'attributes': {'race': 0, 'sex': 0}} gruppi\n",
            "{'name': 'White Female', 'attributes': {'race': 1, 'sex': 0}} {'name': 'Black Female', 'attributes': {'race': 0, 'sex': 0}} gruppi\n",
            "                              Balanced Accuracy  Accuracy  F1 Score  \\\n",
            "Comparison                                                            \n",
            "White Male vs Black Male               0.646666  0.772488  0.464338   \n",
            "White Male vs White Female             0.646666  0.772488  0.464338   \n",
            "White Male vs Black Female             0.646666  0.772488  0.464338   \n",
            "Black Male vs White Female             0.646666  0.772488  0.464338   \n",
            "Black Male vs Black Female             0.646666  0.772488  0.464338   \n",
            "White Female vs Black Female           0.646666  0.772488  0.464338   \n",
            "\n",
            "                              Disparate Impact Ratio  Average Odds Difference  \\\n",
            "Comparison                                                                      \n",
            "White Male vs Black Male                    0.639155                -0.035391   \n",
            "White Male vs White Female                  0.666982                 0.004174   \n",
            "White Male vs Black Female                  0.367922                -0.078706   \n",
            "Black Male vs White Female                  1.043538                 0.039565   \n",
            "Black Male vs Black Female                  0.575638                -0.043314   \n",
            "White Female vs Black Female                0.551621                -0.082879   \n",
            "\n",
            "                              Equal Opportunity Difference  \\\n",
            "Comparison                                                   \n",
            "White Male vs Black Male                         -0.034622   \n",
            "White Male vs White Female                        0.024447   \n",
            "White Male vs Black Female                       -0.105804   \n",
            "Black Male vs White Female                        0.059069   \n",
            "Black Male vs Black Female                       -0.071182   \n",
            "White Female vs Black Female                     -0.130251   \n",
            "\n",
            "                                       Consistency  \n",
            "Comparison                                          \n",
            "White Male vs Black Male      [0.9254559505409572]  \n",
            "White Male vs White Female    [0.9254559505409572]  \n",
            "White Male vs Black Female    [0.9254559505409572]  \n",
            "Black Male vs White Female    [0.9254559505409572]  \n",
            "Black Male vs Black Female    [0.9254559505409572]  \n",
            "White Female vs Black Female  [0.9254559505409572]  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##################################################################################\n",
        "results_dbscan_ratio, pred_labels_dbscan_ratio = evaluate_model_performance(X_train_resampled_dbscan_ratio, X_test, sensitive_attributes, label,\n",
        "                                                            groups, model=model)\n",
        "\n",
        "# Initialize a list to hold DataFrames\n",
        "data_frames = []\n",
        "\n",
        "# Populate the list with DataFrames, each having a unique row index\n",
        "for key, values in results_dbscan_ratio.items():\n",
        "    df_part = pd.DataFrame([values], index=[key])\n",
        "    data_frames.append(df_part)\n",
        "\n",
        "# Concatenate all DataFrames into a single DataFrame\n",
        "results_dbscan_ratio_df = pd.concat(data_frames)\n",
        "results_dbscan_ratio_df.index.name = 'Comparison'\n",
        "\n",
        "# Print the results DataFrame\n",
        "print(results_dbscan_ratio_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v4f8iMJINb43",
        "outputId": "1148daf6-c5f9-45ed-d6a7-a7bf41cc6b7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'name': 'White Male', 'attributes': {'race': 1, 'sex': 1}} {'name': 'Black Male', 'attributes': {'race': 0, 'sex': 1}} gruppi\n",
            "{'name': 'White Male', 'attributes': {'race': 1, 'sex': 1}} {'name': 'White Female', 'attributes': {'race': 1, 'sex': 0}} gruppi\n",
            "{'name': 'White Male', 'attributes': {'race': 1, 'sex': 1}} {'name': 'Black Female', 'attributes': {'race': 0, 'sex': 0}} gruppi\n",
            "{'name': 'Black Male', 'attributes': {'race': 0, 'sex': 1}} {'name': 'White Female', 'attributes': {'race': 1, 'sex': 0}} gruppi\n",
            "{'name': 'Black Male', 'attributes': {'race': 0, 'sex': 1}} {'name': 'Black Female', 'attributes': {'race': 0, 'sex': 0}} gruppi\n",
            "{'name': 'White Female', 'attributes': {'race': 1, 'sex': 0}} {'name': 'Black Female', 'attributes': {'race': 0, 'sex': 0}} gruppi\n",
            "                              Balanced Accuracy  Accuracy  F1 Score  \\\n",
            "Comparison                                                            \n",
            "White Male vs Black Male               0.640086  0.780835  0.449748   \n",
            "White Male vs White Female             0.640086  0.780835  0.449748   \n",
            "White Male vs Black Female             0.640086  0.780835  0.449748   \n",
            "Black Male vs White Female             0.640086  0.780835  0.449748   \n",
            "Black Male vs Black Female             0.640086  0.780835  0.449748   \n",
            "White Female vs Black Female           0.640086  0.780835  0.449748   \n",
            "\n",
            "                              Disparate Impact Ratio  Average Odds Difference  \\\n",
            "Comparison                                                                      \n",
            "White Male vs Black Male                    0.623380                -0.032525   \n",
            "White Male vs White Female                  0.612854                -0.017307   \n",
            "White Male vs Black Female                  0.343249                -0.070781   \n",
            "Black Male vs White Female                  0.983114                 0.015218   \n",
            "Black Male vs Black Female                  0.550625                -0.038256   \n",
            "White Female vs Black Female                0.560083                -0.053474   \n",
            "\n",
            "                              Equal Opportunity Difference  \\\n",
            "Comparison                                                   \n",
            "White Male vs Black Male                         -0.037706   \n",
            "White Male vs White Female                       -0.023832   \n",
            "White Male vs Black Female                       -0.102417   \n",
            "Black Male vs White Female                        0.013874   \n",
            "Black Male vs Black Female                       -0.064711   \n",
            "White Female vs Black Female                     -0.078585   \n",
            "\n",
            "                                       Consistency  \n",
            "Comparison                                          \n",
            "White Male vs Black Male      [0.9353323029366301]  \n",
            "White Male vs White Female    [0.9353323029366301]  \n",
            "White Male vs Black Female    [0.9353323029366301]  \n",
            "Black Male vs White Female    [0.9353323029366301]  \n",
            "Black Male vs Black Female    [0.9353323029366301]  \n",
            "White Female vs Black Female  [0.9353323029366301]  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ModelEvaluator:\n",
        "    def __init__(self, df, protected_attributes, label_name, privileged, unprivileged, fav, unfav, groups, num_iterations=10, oversampling_methods=None):\n",
        "        self.df = df\n",
        "        self.protected_attributes = protected_attributes\n",
        "        self.label_name = label_name\n",
        "        self.privileged = privileged\n",
        "        self.unprivileged = unprivileged\n",
        "        self.fav = fav\n",
        "        self.unfav = unfav\n",
        "        self.groups = groups\n",
        "        self.num_iterations = num_iterations\n",
        "        self.oversampling_methods = oversampling_methods if oversampling_methods is not None else ['none']\n",
        "\n",
        "    def evaluate_model_performance_mean(self):\n",
        "        results_dict = {method: [] for method in self.oversampling_methods}\n",
        "\n",
        "        for _ in range(self.num_iterations):\n",
        "            data_prep = DataPreparation(self.df, self.protected_attributes, self.label_name,\n",
        "                                        self.privileged, self.unprivileged, self.fav, self.unfav)\n",
        "            data_prep.prepare()\n",
        "            data_prep.df = data_prep.df.reset_index(drop=True)\n",
        "            X_train, X_test = data_prep.X_train, data_prep.X_test\n",
        "            X_train = X_train.reset_index(drop=True)\n",
        "            cat_features = data_prep.cat_features\n",
        "            numerical_features = data_prep.numerical_features\n",
        "            reverse_group_mapping = data_prep.create_group_column()\n",
        "            group_counts_train = X_train['Group'].value_counts().sort_index()\n",
        "            subgroup_column_train = X_train['Group']\n",
        "            subgroup_column_test = X_test['Group']\n",
        "            X_train = X_train.drop(columns=['Group'])\n",
        "            X_test = X_test.drop(columns=['Group'])\n",
        "\n",
        "            num_privileged_ones = X_train[(X_train[self.protected_attributes[0]] == 1) &\n",
        "                                          (X_train[self.protected_attributes[1]] == 1) &\n",
        "                                          (X_train[self.label_name] == 1)].shape[0]\n",
        "            num_privileged_zeros = X_train[(X_train[self.protected_attributes[0]] == 1) &\n",
        "                                          (X_train[self.protected_attributes[1]] == 1) &\n",
        "                                          (X_train[self.label_name] == 0)].shape[0]\n",
        "            total_ratio = num_privileged_ones / num_privileged_zeros if num_privileged_zeros != 0 else float('inf')\n",
        "\n",
        "            for method in self.oversampling_methods:\n",
        "                if method == 'none':\n",
        "                    results, pred_labels = evaluate_model_performance(X_train, X_test, self.protected_attributes, self.label_name,\n",
        "                                                                      self.groups, model=model)\n",
        "                elif method == 'custom_smote_dbscan':\n",
        "                    synthetic_samples_matrix_dbscan = oversample_groups(X_train, cat_features, custom_smote_dbscan, subgroup_column_train, self.protected_attributes,\n",
        "                                                                      self.label_name, reverse_group_mapping)\n",
        "                    X_train_resampled_dbscan = pd.concat([X_train, pd.DataFrame(synthetic_samples_matrix_dbscan, columns=X_train.columns)], ignore_index=True)\n",
        "                    results, pred_labels = evaluate_model_performance(X_train_resampled_dbscan, X_test, self.protected_attributes,\n",
        "                                                                      self.label_name, self.groups, model=model)\n",
        "                elif method == 'custom_smote_dbscan_ratio':\n",
        "                    synthetic_samples_matrix_dbscan_ratio, synthetic_samples_group_dbscan_ratio = oversample_groups_ratio(X_train, cat_features, custom_smote_dbscan_ratio, subgroup_column_train, total_ratio, reverse_group_mapping)\n",
        "                    X_train_resampled_dbscan_ratio = pd.concat([X_train, pd.DataFrame(synthetic_samples_matrix_dbscan_ratio, columns=X_train.columns)], ignore_index=True)\n",
        "                    results, pred_labels = evaluate_model_performance(X_train_resampled_dbscan_ratio, X_test, self.protected_attributes,\n",
        "                                                                      self.label_name, self.groups, model=model)\n",
        "\n",
        "                data_frames = []\n",
        "                for key, values in results.items():\n",
        "                    df_part = pd.DataFrame([values], index=[key])\n",
        "                    data_frames.append(df_part)\n",
        "\n",
        "                results_df = pd.concat(data_frames)\n",
        "                results_df.index.name = 'Comparison'\n",
        "                results_dict[method].append(results_df)\n",
        "\n",
        "        combined_results = {method: pd.concat(results_dict[method]).groupby(level=0).mean() for method in self.oversampling_methods}\n",
        "        return combined_results\n"
      ],
      "metadata": {
        "id": "ON1e5ctAaCvW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluators = {\n",
        "    'ModelEvaluator': ModelEvaluator(df, sensitive_attributes, label, privileged, unprivileged, favorable_label, unfavorable_label, groups, num_iterations=2, oversampling_methods=['none', 'custom_smote_dbscan', 'custom_smote_dbscan_ratio'])\n",
        "}\n",
        "\n",
        "results = {}\n",
        "\n",
        "# Evaluate all models and store results\n",
        "for key, evaluator in evaluators.items():\n",
        "    results[key] = evaluator.evaluate_model_performance_mean()\n",
        "\n",
        "# Display the results\n",
        "for method, result in results['ModelEvaluator'].items():\n",
        "    print(f\"Results for {method} method:\")\n",
        "    print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jglKZ5qyabE3",
        "outputId": "59e263a0-e671-4511-9110-fcdc7712c172"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected 3316 rows with missing values. Removed them.\n",
            "The 'income' column has only two unique values.\n",
            "{'workclass': {'Federal-gov': 0, 'Local-gov': 1, 'Private': 2, 'Self-emp-inc': 3, 'Self-emp-not-inc': 4, 'State-gov': 5, 'Without-pay': 6}, 'education': {'10th': 0, '11th': 1, '12th': 2, '1st-4th': 3, '5th-6th': 4, '7th-8th': 5, '9th': 6, 'Assoc-acdm': 7, 'Assoc-voc': 8, 'Bachelors': 9, 'Doctorate': 10, 'HS-grad': 11, 'Masters': 12, 'Preschool': 13, 'Prof-school': 14, 'Some-college': 15}, 'marital_status': {'Divorced': 0, 'Married-AF-spouse': 1, 'Married-civ-spouse': 2, 'Married-spouse-absent': 3, 'Never-married': 4, 'Separated': 5, 'Widowed': 6}, 'occupation': {'Adm-clerical': 0, 'Armed-Forces': 1, 'Craft-repair': 2, 'Exec-managerial': 3, 'Farming-fishing': 4, 'Handlers-cleaners': 5, 'Machine-op-inspct': 6, 'Other-service': 7, 'Priv-house-serv': 8, 'Prof-specialty': 9, 'Protective-serv': 10, 'Sales': 11, 'Tech-support': 12, 'Transport-moving': 13}, 'relationship': {'Husband': 0, 'Not-in-family': 1, 'Other-relative': 2, 'Own-child': 3, 'Unmarried': 4, 'Wife': 5}, 'race': {0: 0, 1: 1}, 'sex': {0: 0, 1: 1}, 'native_country': {'Cambodia': 0, 'Canada': 1, 'China': 2, 'Columbia': 3, 'Cuba': 4, 'Dominican-Republic': 5, 'Ecuador': 6, 'El-Salvador': 7, 'England': 8, 'France': 9, 'Germany': 10, 'Greece': 11, 'Guatemala': 12, 'Haiti': 13, 'Holand-Netherlands': 14, 'Honduras': 15, 'Hong': 16, 'Hungary': 17, 'India': 18, 'Iran': 19, 'Ireland': 20, 'Italy': 21, 'Jamaica': 22, 'Japan': 23, 'Mexico': 24, 'Nicaragua': 25, 'Outlying-US(Guam-USVI-etc)': 26, 'Peru': 27, 'Philippines': 28, 'Poland': 29, 'Portugal': 30, 'Puerto-Rico': 31, 'Scotland': 32, 'South': 33, 'Taiwan': 34, 'Thailand': 35, 'Trinadad&Tobago': 36, 'United-States': 37, 'Vietnam': 38, 'Yugoslavia': 39}, 'income': {0: 0, 1: 1}} encoder dict\n",
            "[(0, (1, 1, 0)), (1, (1, 1, 1)), (2, (1, 0, 0)), (3, (1, 0, 1)), (4, (0, 1, 0)), (5, (0, 1, 1)), (6, (0, 0, 0)), (7, (0, 0, 1))]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-83-379c487b634e>:129: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  self.df['Group'] = pd.MultiIndex.from_frame(self.df[self.sensitive + [self.label]]).map(group_mapping)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(0, (1, 1, 0)), (1, (1, 1, 1)), (2, (1, 0, 0)), (3, (1, 0, 1)), (4, (0, 1, 0)), (5, (0, 1, 1)), (6, (0, 0, 0)), (7, (0, 0, 1))]\n",
            "{'name': 'White Male', 'attributes': {'race': 1, 'sex': 1}} {'name': 'Black Male', 'attributes': {'race': 0, 'sex': 1}} gruppi\n",
            "{'name': 'White Male', 'attributes': {'race': 1, 'sex': 1}} {'name': 'White Female', 'attributes': {'race': 1, 'sex': 0}} gruppi\n",
            "{'name': 'White Male', 'attributes': {'race': 1, 'sex': 1}} {'name': 'Black Female', 'attributes': {'race': 0, 'sex': 0}} gruppi\n",
            "{'name': 'Black Male', 'attributes': {'race': 0, 'sex': 1}} {'name': 'White Female', 'attributes': {'race': 1, 'sex': 0}} gruppi\n",
            "{'name': 'Black Male', 'attributes': {'race': 0, 'sex': 1}} {'name': 'Black Female', 'attributes': {'race': 0, 'sex': 0}} gruppi\n",
            "{'name': 'White Female', 'attributes': {'race': 1, 'sex': 0}} {'name': 'Black Female', 'attributes': {'race': 0, 'sex': 0}} gruppi\n",
            "0.555 1 [0] mids\n",
            "0.01 0.555 min, max\n",
            "0.28250000000000003 1 [0] mids\n",
            "0.01 0.28250000000000003 min, max\n",
            "0.14625000000000002 2 [-1  0] mids\n",
            "0.01 0.14625000000000002 min, max\n",
            "0.07812500000000001 2 [-1  0] mids\n",
            "0.01 0.07812500000000001 min, max\n",
            "0.044062500000000004 150 [ -1   0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16\n",
            "  17  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34\n",
            "  35  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52\n",
            "  53  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70\n",
            "  71  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88\n",
            "  89  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106\n",
            " 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124\n",
            " 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142\n",
            " 143 144 145 146 147 148] mids\n",
            "0.044062500000000004 0.07812500000000001 min, max\n",
            "0.06109375000000001 152 [ -1   0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16\n",
            "  17  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34\n",
            "  35  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52\n",
            "  53  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70\n",
            "  71  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88\n",
            "  89  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106\n",
            " 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124\n",
            " 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142\n",
            " 143 144 145 146 147 148 149 150] mids\n",
            "0.06109375000000001 0.07812500000000001 min, max\n",
            "0.06960937500000001 152 [ -1   0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16\n",
            "  17  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34\n",
            "  35  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52\n",
            "  53  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70\n",
            "  71  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88\n",
            "  89  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106\n",
            " 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124\n",
            " 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142\n",
            " 143 144 145 146 147 148 149 150] mids\n",
            "0.06960937500000001 0.07812500000000001 min, max\n",
            "0.07386718750000001 152 [ -1   0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16\n",
            "  17  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34\n",
            "  35  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52\n",
            "  53  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70\n",
            "  71  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88\n",
            "  89  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106\n",
            " 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124\n",
            " 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142\n",
            " 143 144 145 146 147 148 149 150] mids\n",
            "0.07386718750000001 0.07812500000000001 min, max\n",
            "0.07599609375000002 152 [ -1   0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16\n",
            "  17  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34\n",
            "  35  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52\n",
            "  53  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70\n",
            "  71  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88\n",
            "  89  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106\n",
            " 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124\n",
            " 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142\n",
            " 143 144 145 146 147 148 149 150] mids\n",
            "0.07599609375000002 0.07812500000000001 min, max\n",
            "0.07706054687500002 3 [-1  0  1] mids\n",
            "0.07706054687500002 0.07812500000000001 min, max\n",
            "0.07759277343750001 3 [-1  0  1] mids\n",
            "0.07759277343750001 0.07812500000000001 min, max\n",
            "Oversampling for group  (2): Added 5488 synthetic samples in 2.\n",
            "0.555 1 [0] mids\n",
            "0.01 0.555 min, max\n",
            "0.28250000000000003 1 [0] mids\n",
            "0.01 0.28250000000000003 min, max\n",
            "0.14625000000000002 3 [-1  0  1] mids\n",
            "0.14625000000000002 0.28250000000000003 min, max\n",
            "0.21437500000000004 2 [-1  0] mids\n",
            "0.14625000000000002 0.21437500000000004 min, max\n",
            "0.18031250000000004 2 [-1  0] mids\n",
            "0.14625000000000002 0.18031250000000004 min, max\n",
            "0.16328125000000004 2 [-1  0] mids\n",
            "0.14625000000000002 0.16328125000000004 min, max\n",
            "0.15476562500000002 2 [-1  0] mids\n",
            "0.14625000000000002 0.15476562500000002 min, max\n",
            "0.1505078125 3 [-1  0  1] mids\n",
            "0.1505078125 0.15476562500000002 min, max\n",
            "0.15263671875 3 [-1  0  1] mids\n",
            "0.15263671875 0.15476562500000002 min, max\n",
            "0.15370117187500001 3 [-1  0  1] mids\n",
            "0.15370117187500001 0.15476562500000002 min, max\n",
            "0.1542333984375 2 [-1  0] mids\n",
            "0.15370117187500001 0.1542333984375 min, max\n",
            "Oversampling for group  (3): Added 5107 synthetic samples in 3.\n",
            "0.555 1 [0] mids\n",
            "0.01 0.555 min, max\n",
            "0.28250000000000003 1 [0] mids\n",
            "0.01 0.28250000000000003 min, max\n",
            "0.14625000000000002 2 [-1  0] mids\n",
            "0.01 0.14625000000000002 min, max\n",
            "0.07812500000000001 4 [-1  0  1  2] mids\n",
            "0.07812500000000001 0.14625000000000002 min, max\n",
            "0.11218750000000002 2 [-1  0] mids\n",
            "0.07812500000000001 0.11218750000000002 min, max\n",
            "0.09515625000000003 2 [-1  0] mids\n",
            "0.07812500000000001 0.09515625000000003 min, max\n",
            "0.08664062500000003 3 [-1  0  1] mids\n",
            "0.08664062500000003 0.09515625000000003 min, max\n",
            "0.09089843750000003 2 [-1  0] mids\n",
            "0.08664062500000003 0.09089843750000003 min, max\n",
            "0.08876953125000003 2 [-1  0] mids\n",
            "0.08664062500000003 0.08876953125000003 min, max\n",
            "0.08770507812500003 3 [-1  0  1] mids\n",
            "0.08770507812500003 0.08876953125000003 min, max\n",
            "0.08823730468750003 3 [-1  0  1] mids\n",
            "0.08823730468750003 0.08876953125000003 min, max\n",
            "Oversampling for group  (4): Added 11572 synthetic samples in 4.\n",
            "0.555 1 [0] mids\n",
            "0.01 0.555 min, max\n",
            "0.28250000000000003 1 [0] mids\n",
            "0.01 0.28250000000000003 min, max\n",
            "0.14625000000000002 3 [-1  0  1] mids\n",
            "0.14625000000000002 0.28250000000000003 min, max\n",
            "0.21437500000000004 2 [-1  0] mids\n",
            "0.14625000000000002 0.21437500000000004 min, max\n",
            "0.18031250000000004 2 [-1  0] mids\n",
            "0.14625000000000002 0.18031250000000004 min, max\n",
            "0.16328125000000004 2 [-1  0] mids\n",
            "0.14625000000000002 0.16328125000000004 min, max\n",
            "0.15476562500000002 3 [-1  0  1] mids\n",
            "0.15476562500000002 0.16328125000000004 min, max\n",
            "0.15902343750000003 2 [-1  0] mids\n",
            "0.15476562500000002 0.15902343750000003 min, max\n",
            "0.15689453125000002 2 [-1  0] mids\n",
            "0.15476562500000002 0.15689453125000002 min, max\n",
            "0.15583007812500002 2 [-1  0] mids\n",
            "0.15476562500000002 0.15583007812500002 min, max\n",
            "0.15529785156250003 2 [-1  0] mids\n",
            "0.15476562500000002 0.15529785156250003 min, max\n",
            "Oversampling for group  (5): Added 5840 synthetic samples in 5.\n",
            "0.555 1 [0] mids\n",
            "0.01 0.555 min, max\n",
            "0.28250000000000003 1 [0] mids\n",
            "0.01 0.28250000000000003 min, max\n",
            "0.14625000000000002 2 [-1  0] mids\n",
            "0.01 0.14625000000000002 min, max\n",
            "0.07812500000000001 4 [-1  0  1  2] mids\n",
            "0.07812500000000001 0.14625000000000002 min, max\n",
            "0.11218750000000002 2 [-1  0] mids\n",
            "0.07812500000000001 0.11218750000000002 min, max\n",
            "0.09515625000000003 2 [-1  0] mids\n",
            "0.07812500000000001 0.09515625000000003 min, max\n",
            "0.08664062500000003 3 [-1  0  1] mids\n",
            "0.08664062500000003 0.09515625000000003 min, max\n",
            "0.09089843750000003 2 [-1  0] mids\n",
            "0.08664062500000003 0.09089843750000003 min, max\n",
            "0.08876953125000003 2 [-1  0] mids\n",
            "0.08664062500000003 0.08876953125000003 min, max\n",
            "0.08770507812500003 3 [-1  0  1] mids\n",
            "0.08770507812500003 0.08876953125000003 min, max\n",
            "0.08823730468750003 3 [-1  0  1] mids\n",
            "0.08823730468750003 0.08876953125000003 min, max\n",
            "Oversampling for group  (6): Added 11416 synthetic samples in 6.\n",
            "0.555 1 [0] mids\n",
            "0.01 0.555 min, max\n",
            "0.28250000000000003 1 [0] mids\n",
            "0.01 0.28250000000000003 min, max\n",
            "0.14625000000000002 4 [-1  0  1  2] mids\n",
            "0.14625000000000002 0.28250000000000003 min, max\n",
            "0.21437500000000004 2 [-1  0] mids\n",
            "0.14625000000000002 0.21437500000000004 min, max\n",
            "0.18031250000000004 2 [-1  0] mids\n",
            "0.14625000000000002 0.18031250000000004 min, max\n",
            "0.16328125000000004 2 [-1  0] mids\n",
            "0.14625000000000002 0.16328125000000004 min, max\n",
            "0.15476562500000002 4 [-1  0  1  2] mids\n",
            "0.15476562500000002 0.16328125000000004 min, max\n",
            "0.15902343750000003 2 [-1  0] mids\n",
            "0.15476562500000002 0.15902343750000003 min, max\n",
            "0.15689453125000002 3 [-1  0  1] mids\n",
            "0.15689453125000002 0.15902343750000003 min, max\n",
            "0.15795898437500003 2 [-1  0] mids\n",
            "0.15689453125000002 0.15795898437500003 min, max\n",
            "0.15742675781250004 2 [-1  0] mids\n",
            "0.15689453125000002 0.15742675781250004 min, max\n",
            "Oversampling for group  (7): Added 6038 synthetic samples in 7.\n",
            "{'name': 'White Male', 'attributes': {'race': 1, 'sex': 1}} {'name': 'Black Male', 'attributes': {'race': 0, 'sex': 1}} gruppi\n",
            "{'name': 'White Male', 'attributes': {'race': 1, 'sex': 1}} {'name': 'White Female', 'attributes': {'race': 1, 'sex': 0}} gruppi\n",
            "{'name': 'White Male', 'attributes': {'race': 1, 'sex': 1}} {'name': 'Black Female', 'attributes': {'race': 0, 'sex': 0}} gruppi\n",
            "{'name': 'Black Male', 'attributes': {'race': 0, 'sex': 1}} {'name': 'White Female', 'attributes': {'race': 1, 'sex': 0}} gruppi\n",
            "{'name': 'Black Male', 'attributes': {'race': 0, 'sex': 1}} {'name': 'Black Female', 'attributes': {'race': 0, 'sex': 0}} gruppi\n",
            "{'name': 'White Female', 'attributes': {'race': 1, 'sex': 0}} {'name': 'Black Female', 'attributes': {'race': 0, 'sex': 0}} gruppi\n",
            "0.555 1 [0] mids\n",
            "0.01 0.555 min, max\n",
            "0.28250000000000003 1 [0] mids\n",
            "0.01 0.28250000000000003 min, max\n",
            "0.14625000000000002 3 [-1  0  1] mids\n",
            "0.14625000000000002 0.28250000000000003 min, max\n",
            "0.21437500000000004 2 [-1  0] mids\n",
            "0.14625000000000002 0.21437500000000004 min, max\n",
            "0.18031250000000004 2 [-1  0] mids\n",
            "0.14625000000000002 0.18031250000000004 min, max\n",
            "0.16328125000000004 2 [-1  0] mids\n",
            "0.14625000000000002 0.16328125000000004 min, max\n",
            "0.15476562500000002 2 [-1  0] mids\n",
            "0.14625000000000002 0.15476562500000002 min, max\n",
            "0.1505078125 3 [-1  0  1] mids\n",
            "0.1505078125 0.15476562500000002 min, max\n",
            "0.15263671875 3 [-1  0  1] mids\n",
            "0.15263671875 0.15476562500000002 min, max\n",
            "0.15370117187500001 3 [-1  0  1] mids\n",
            "0.15370117187500001 0.15476562500000002 min, max\n",
            "0.1542333984375 2 [-1  0] mids\n",
            "0.15370117187500001 0.1542333984375 min, max\n",
            "Oversampling for group pair (3, 2): Added 2478 synthetic samples in 3.\n",
            "0.555 1 [0] mids\n",
            "0.01 0.555 min, max\n",
            "0.28250000000000003 1 [0] mids\n",
            "0.01 0.28250000000000003 min, max\n",
            "0.14625000000000002 3 [-1  0  1] mids\n",
            "0.14625000000000002 0.28250000000000003 min, max\n",
            "0.21437500000000004 2 [-1  0] mids\n",
            "0.14625000000000002 0.21437500000000004 min, max\n",
            "0.18031250000000004 2 [-1  0] mids\n",
            "0.14625000000000002 0.18031250000000004 min, max\n",
            "0.16328125000000004 2 [-1  0] mids\n",
            "0.14625000000000002 0.16328125000000004 min, max\n",
            "0.15476562500000002 3 [-1  0  1] mids\n",
            "0.15476562500000002 0.16328125000000004 min, max\n",
            "0.15902343750000003 2 [-1  0] mids\n",
            "0.15476562500000002 0.15902343750000003 min, max\n",
            "0.15689453125000002 2 [-1  0] mids\n",
            "0.15476562500000002 0.15689453125000002 min, max\n",
            "0.15583007812500002 2 [-1  0] mids\n",
            "0.15476562500000002 0.15583007812500002 min, max\n",
            "0.15529785156250003 2 [-1  0] mids\n",
            "0.15476562500000002 0.15529785156250003 min, max\n",
            "Oversampling for group pair (5, 4): Added 297 synthetic samples in 5.\n",
            "0.555 1 [0] mids\n",
            "0.01 0.555 min, max\n",
            "0.28250000000000003 1 [0] mids\n",
            "0.01 0.28250000000000003 min, max\n",
            "0.14625000000000002 4 [-1  0  1  2] mids\n",
            "0.14625000000000002 0.28250000000000003 min, max\n",
            "0.21437500000000004 2 [-1  0] mids\n",
            "0.14625000000000002 0.21437500000000004 min, max\n",
            "0.18031250000000004 2 [-1  0] mids\n",
            "0.14625000000000002 0.18031250000000004 min, max\n",
            "0.16328125000000004 2 [-1  0] mids\n",
            "0.14625000000000002 0.16328125000000004 min, max\n",
            "0.15476562500000002 4 [-1  0  1  2] mids\n",
            "0.15476562500000002 0.16328125000000004 min, max\n",
            "0.15902343750000003 2 [-1  0] mids\n",
            "0.15476562500000002 0.15902343750000003 min, max\n",
            "0.15689453125000002 3 [-1  0  1] mids\n",
            "0.15689453125000002 0.15902343750000003 min, max\n",
            "0.15795898437500003 2 [-1  0] mids\n",
            "0.15689453125000002 0.15795898437500003 min, max\n",
            "0.15742675781250004 2 [-1  0] mids\n",
            "0.15689453125000002 0.15742675781250004 min, max\n",
            "Oversampling for group pair (7, 6): Added 569 synthetic samples in 7.\n",
            "{'name': 'White Male', 'attributes': {'race': 1, 'sex': 1}} {'name': 'Black Male', 'attributes': {'race': 0, 'sex': 1}} gruppi\n",
            "{'name': 'White Male', 'attributes': {'race': 1, 'sex': 1}} {'name': 'White Female', 'attributes': {'race': 1, 'sex': 0}} gruppi\n",
            "{'name': 'White Male', 'attributes': {'race': 1, 'sex': 1}} {'name': 'Black Female', 'attributes': {'race': 0, 'sex': 0}} gruppi\n",
            "{'name': 'Black Male', 'attributes': {'race': 0, 'sex': 1}} {'name': 'White Female', 'attributes': {'race': 1, 'sex': 0}} gruppi\n",
            "{'name': 'Black Male', 'attributes': {'race': 0, 'sex': 1}} {'name': 'Black Female', 'attributes': {'race': 0, 'sex': 0}} gruppi\n",
            "{'name': 'White Female', 'attributes': {'race': 1, 'sex': 0}} {'name': 'Black Female', 'attributes': {'race': 0, 'sex': 0}} gruppi\n",
            "Detected 3316 rows with missing values. Removed them.\n",
            "The 'income' column has only two unique values.\n",
            "{'workclass': {'Federal-gov': 0, 'Local-gov': 1, 'Private': 2, 'Self-emp-inc': 3, 'Self-emp-not-inc': 4, 'State-gov': 5, 'Without-pay': 6}, 'education': {'10th': 0, '11th': 1, '12th': 2, '1st-4th': 3, '5th-6th': 4, '7th-8th': 5, '9th': 6, 'Assoc-acdm': 7, 'Assoc-voc': 8, 'Bachelors': 9, 'Doctorate': 10, 'HS-grad': 11, 'Masters': 12, 'Preschool': 13, 'Prof-school': 14, 'Some-college': 15}, 'marital_status': {'Divorced': 0, 'Married-AF-spouse': 1, 'Married-civ-spouse': 2, 'Married-spouse-absent': 3, 'Never-married': 4, 'Separated': 5, 'Widowed': 6}, 'occupation': {'Adm-clerical': 0, 'Armed-Forces': 1, 'Craft-repair': 2, 'Exec-managerial': 3, 'Farming-fishing': 4, 'Handlers-cleaners': 5, 'Machine-op-inspct': 6, 'Other-service': 7, 'Priv-house-serv': 8, 'Prof-specialty': 9, 'Protective-serv': 10, 'Sales': 11, 'Tech-support': 12, 'Transport-moving': 13}, 'relationship': {'Husband': 0, 'Not-in-family': 1, 'Other-relative': 2, 'Own-child': 3, 'Unmarried': 4, 'Wife': 5}, 'race': {0: 0, 1: 1}, 'sex': {0: 0, 1: 1}, 'native_country': {'Cambodia': 0, 'Canada': 1, 'China': 2, 'Columbia': 3, 'Cuba': 4, 'Dominican-Republic': 5, 'Ecuador': 6, 'El-Salvador': 7, 'England': 8, 'France': 9, 'Germany': 10, 'Greece': 11, 'Guatemala': 12, 'Haiti': 13, 'Holand-Netherlands': 14, 'Honduras': 15, 'Hong': 16, 'Hungary': 17, 'India': 18, 'Iran': 19, 'Ireland': 20, 'Italy': 21, 'Jamaica': 22, 'Japan': 23, 'Mexico': 24, 'Nicaragua': 25, 'Outlying-US(Guam-USVI-etc)': 26, 'Peru': 27, 'Philippines': 28, 'Poland': 29, 'Portugal': 30, 'Puerto-Rico': 31, 'Scotland': 32, 'South': 33, 'Taiwan': 34, 'Thailand': 35, 'Trinadad&Tobago': 36, 'United-States': 37, 'Vietnam': 38, 'Yugoslavia': 39}, 'income': {0: 0, 1: 1}} encoder dict\n",
            "[(0, (1, 1, 0)), (1, (1, 1, 1)), (2, (1, 0, 0)), (3, (1, 0, 1)), (4, (0, 1, 0)), (5, (0, 1, 1)), (6, (0, 0, 0)), (7, (0, 0, 1))]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-83-379c487b634e>:129: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  self.df['Group'] = pd.MultiIndex.from_frame(self.df[self.sensitive + [self.label]]).map(group_mapping)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(0, (1, 1, 0)), (1, (1, 1, 1)), (2, (1, 0, 0)), (3, (1, 0, 1)), (4, (0, 1, 0)), (5, (0, 1, 1)), (6, (0, 0, 0)), (7, (0, 0, 1))]\n",
            "{'name': 'White Male', 'attributes': {'race': 1, 'sex': 1}} {'name': 'Black Male', 'attributes': {'race': 0, 'sex': 1}} gruppi\n",
            "{'name': 'White Male', 'attributes': {'race': 1, 'sex': 1}} {'name': 'White Female', 'attributes': {'race': 1, 'sex': 0}} gruppi\n",
            "{'name': 'White Male', 'attributes': {'race': 1, 'sex': 1}} {'name': 'Black Female', 'attributes': {'race': 0, 'sex': 0}} gruppi\n",
            "{'name': 'Black Male', 'attributes': {'race': 0, 'sex': 1}} {'name': 'White Female', 'attributes': {'race': 1, 'sex': 0}} gruppi\n",
            "{'name': 'Black Male', 'attributes': {'race': 0, 'sex': 1}} {'name': 'Black Female', 'attributes': {'race': 0, 'sex': 0}} gruppi\n",
            "{'name': 'White Female', 'attributes': {'race': 1, 'sex': 0}} {'name': 'Black Female', 'attributes': {'race': 0, 'sex': 0}} gruppi\n",
            "0.555 1 [0] mids\n",
            "0.01 0.555 min, max\n",
            "0.28250000000000003 1 [0] mids\n",
            "0.01 0.28250000000000003 min, max\n",
            "0.14625000000000002 2 [-1  0] mids\n",
            "0.01 0.14625000000000002 min, max\n",
            "0.07812500000000001 3 [-1  0  1] mids\n",
            "0.07812500000000001 0.14625000000000002 min, max\n",
            "0.11218750000000002 2 [-1  0] mids\n",
            "0.07812500000000001 0.11218750000000002 min, max\n",
            "0.09515625000000003 2 [-1  0] mids\n",
            "0.07812500000000001 0.09515625000000003 min, max\n",
            "0.08664062500000003 2 [-1  0] mids\n",
            "0.07812500000000001 0.08664062500000003 min, max\n",
            "0.08238281250000001 2 [-1  0] mids\n",
            "0.07812500000000001 0.08238281250000001 min, max\n",
            "0.08025390625 3 [-1  0  1] mids\n",
            "0.08025390625 0.08238281250000001 min, max\n",
            "0.08131835937500001 2 [-1  0] mids\n",
            "0.08025390625 0.08131835937500001 min, max\n",
            "0.08078613281250001 3 [-1  0  1] mids\n",
            "0.08078613281250001 0.08131835937500001 min, max\n",
            "Oversampling for group  (2): Added 5488 synthetic samples in 2.\n",
            "0.555 1 [0] mids\n",
            "0.01 0.555 min, max\n",
            "0.28250000000000003 1 [0] mids\n",
            "0.01 0.28250000000000003 min, max\n",
            "0.14625000000000002 3 [-1  0  1] mids\n",
            "0.14625000000000002 0.28250000000000003 min, max\n",
            "0.21437500000000004 2 [-1  0] mids\n",
            "0.14625000000000002 0.21437500000000004 min, max\n",
            "0.18031250000000004 2 [-1  0] mids\n",
            "0.14625000000000002 0.18031250000000004 min, max\n",
            "0.16328125000000004 2 [-1  0] mids\n",
            "0.14625000000000002 0.16328125000000004 min, max\n",
            "0.15476562500000002 2 [-1  0] mids\n",
            "0.14625000000000002 0.15476562500000002 min, max\n",
            "0.1505078125 3 [-1  0  1] mids\n",
            "0.1505078125 0.15476562500000002 min, max\n",
            "0.15263671875 3 [-1  0  1] mids\n",
            "0.15263671875 0.15476562500000002 min, max\n",
            "0.15370117187500001 3 [-1  0  1] mids\n",
            "0.15370117187500001 0.15476562500000002 min, max\n",
            "0.1542333984375 2 [-1  0] mids\n",
            "0.15370117187500001 0.1542333984375 min, max\n",
            "Oversampling for group  (3): Added 5107 synthetic samples in 3.\n",
            "0.555 1 [0] mids\n",
            "0.01 0.555 min, max\n",
            "0.28250000000000003 1 [0] mids\n",
            "0.01 0.28250000000000003 min, max\n",
            "0.14625000000000002 2 [-1  0] mids\n",
            "0.01 0.14625000000000002 min, max\n",
            "0.07812500000000001 6 [-1  0  1  2  3  4] mids\n",
            "0.07812500000000001 0.14625000000000002 min, max\n",
            "0.11218750000000002 2 [-1  0] mids\n",
            "0.07812500000000001 0.11218750000000002 min, max\n",
            "0.09515625000000003 2 [-1  0] mids\n",
            "0.07812500000000001 0.09515625000000003 min, max\n",
            "0.08664062500000003 3 [-1  0  1] mids\n",
            "0.08664062500000003 0.09515625000000003 min, max\n",
            "0.09089843750000003 2 [-1  0] mids\n",
            "0.08664062500000003 0.09089843750000003 min, max\n",
            "0.08876953125000003 3 [-1  0  1] mids\n",
            "0.08876953125000003 0.09089843750000003 min, max\n",
            "0.08983398437500004 2 [-1  0] mids\n",
            "0.08876953125000003 0.08983398437500004 min, max\n",
            "0.08930175781250003 3 [-1  0  1] mids\n",
            "0.08930175781250003 0.08983398437500004 min, max\n",
            "Oversampling for group  (4): Added 11572 synthetic samples in 4.\n",
            "0.555 1 [0] mids\n",
            "0.01 0.555 min, max\n",
            "0.28250000000000003 1 [0] mids\n",
            "0.01 0.28250000000000003 min, max\n",
            "0.14625000000000002 3 [-1  0  1] mids\n",
            "0.14625000000000002 0.28250000000000003 min, max\n",
            "0.21437500000000004 2 [-1  0] mids\n",
            "0.14625000000000002 0.21437500000000004 min, max\n",
            "0.18031250000000004 2 [-1  0] mids\n",
            "0.14625000000000002 0.18031250000000004 min, max\n",
            "0.16328125000000004 2 [-1  0] mids\n",
            "0.14625000000000002 0.16328125000000004 min, max\n",
            "0.15476562500000002 3 [-1  0  1] mids\n",
            "0.15476562500000002 0.16328125000000004 min, max\n",
            "0.15902343750000003 3 [-1  0  1] mids\n",
            "0.15902343750000003 0.16328125000000004 min, max\n",
            "0.16115234375000004 2 [-1  0] mids\n",
            "0.15902343750000003 0.16115234375000004 min, max\n",
            "0.16008789062500003 3 [-1  0  1] mids\n",
            "0.16008789062500003 0.16115234375000004 min, max\n",
            "0.16062011718750002 2 [-1  0] mids\n",
            "0.16008789062500003 0.16062011718750002 min, max\n",
            "Oversampling for group  (5): Added 5840 synthetic samples in 5.\n",
            "0.555 1 [0] mids\n",
            "0.01 0.555 min, max\n",
            "0.28250000000000003 1 [0] mids\n",
            "0.01 0.28250000000000003 min, max\n",
            "0.14625000000000002 2 [-1  0] mids\n",
            "0.01 0.14625000000000002 min, max\n",
            "0.07812500000000001 4 [-1  0  1  2] mids\n",
            "0.07812500000000001 0.14625000000000002 min, max\n",
            "0.11218750000000002 2 [-1  0] mids\n",
            "0.07812500000000001 0.11218750000000002 min, max\n",
            "0.09515625000000003 2 [-1  0] mids\n",
            "0.07812500000000001 0.09515625000000003 min, max\n",
            "0.08664062500000003 3 [-1  0  1] mids\n",
            "0.08664062500000003 0.09515625000000003 min, max\n",
            "0.09089843750000003 2 [-1  0] mids\n",
            "0.08664062500000003 0.09089843750000003 min, max\n",
            "0.08876953125000003 2 [-1  0] mids\n",
            "0.08664062500000003 0.08876953125000003 min, max\n",
            "0.08770507812500003 2 [-1  0] mids\n",
            "0.08664062500000003 0.08770507812500003 min, max\n",
            "0.08717285156250003 2 [-1  0] mids\n",
            "0.08664062500000003 0.08717285156250003 min, max\n",
            "Oversampling for group  (6): Added 11416 synthetic samples in 6.\n",
            "0.555 1 [0] mids\n",
            "0.01 0.555 min, max\n",
            "0.28250000000000003 1 [0] mids\n",
            "0.01 0.28250000000000003 min, max\n",
            "0.14625000000000002 3 [-1  0  1] mids\n",
            "0.14625000000000002 0.28250000000000003 min, max\n",
            "0.21437500000000004 2 [-1  0] mids\n",
            "0.14625000000000002 0.21437500000000004 min, max\n",
            "0.18031250000000004 2 [-1  0] mids\n",
            "0.14625000000000002 0.18031250000000004 min, max\n",
            "0.16328125000000004 3 [-1  0  1] mids\n",
            "0.16328125000000004 0.18031250000000004 min, max\n",
            "0.17179687500000004 3 [-1  0  1] mids\n",
            "0.17179687500000004 0.18031250000000004 min, max\n",
            "0.17605468750000003 3 [-1  0  1] mids\n",
            "0.17605468750000003 0.18031250000000004 min, max\n",
            "0.17818359375000004 2 [-1  0] mids\n",
            "0.17605468750000003 0.17818359375000004 min, max\n",
            "0.17711914062500003 2 [-1  0] mids\n",
            "0.17605468750000003 0.17711914062500003 min, max\n",
            "0.17658691406250004 3 [-1  0  1] mids\n",
            "0.17658691406250004 0.17711914062500003 min, max\n",
            "Oversampling for group  (7): Added 6038 synthetic samples in 7.\n",
            "{'name': 'White Male', 'attributes': {'race': 1, 'sex': 1}} {'name': 'Black Male', 'attributes': {'race': 0, 'sex': 1}} gruppi\n",
            "{'name': 'White Male', 'attributes': {'race': 1, 'sex': 1}} {'name': 'White Female', 'attributes': {'race': 1, 'sex': 0}} gruppi\n",
            "{'name': 'White Male', 'attributes': {'race': 1, 'sex': 1}} {'name': 'Black Female', 'attributes': {'race': 0, 'sex': 0}} gruppi\n",
            "{'name': 'Black Male', 'attributes': {'race': 0, 'sex': 1}} {'name': 'White Female', 'attributes': {'race': 1, 'sex': 0}} gruppi\n",
            "{'name': 'Black Male', 'attributes': {'race': 0, 'sex': 1}} {'name': 'Black Female', 'attributes': {'race': 0, 'sex': 0}} gruppi\n",
            "{'name': 'White Female', 'attributes': {'race': 1, 'sex': 0}} {'name': 'Black Female', 'attributes': {'race': 0, 'sex': 0}} gruppi\n",
            "0.555 1 [0] mids\n",
            "0.01 0.555 min, max\n",
            "0.28250000000000003 1 [0] mids\n",
            "0.01 0.28250000000000003 min, max\n",
            "0.14625000000000002 3 [-1  0  1] mids\n",
            "0.14625000000000002 0.28250000000000003 min, max\n",
            "0.21437500000000004 2 [-1  0] mids\n",
            "0.14625000000000002 0.21437500000000004 min, max\n",
            "0.18031250000000004 2 [-1  0] mids\n",
            "0.14625000000000002 0.18031250000000004 min, max\n",
            "0.16328125000000004 2 [-1  0] mids\n",
            "0.14625000000000002 0.16328125000000004 min, max\n",
            "0.15476562500000002 2 [-1  0] mids\n",
            "0.14625000000000002 0.15476562500000002 min, max\n",
            "0.1505078125 3 [-1  0  1] mids\n",
            "0.1505078125 0.15476562500000002 min, max\n",
            "0.15263671875 3 [-1  0  1] mids\n",
            "0.15263671875 0.15476562500000002 min, max\n",
            "0.15370117187500001 3 [-1  0  1] mids\n",
            "0.15370117187500001 0.15476562500000002 min, max\n",
            "0.1542333984375 2 [-1  0] mids\n",
            "0.15370117187500001 0.1542333984375 min, max\n",
            "Oversampling for group pair (3, 2): Added 2478 synthetic samples in 3.\n",
            "0.555 1 [0] mids\n",
            "0.01 0.555 min, max\n",
            "0.28250000000000003 1 [0] mids\n",
            "0.01 0.28250000000000003 min, max\n",
            "0.14625000000000002 3 [-1  0  1] mids\n",
            "0.14625000000000002 0.28250000000000003 min, max\n",
            "0.21437500000000004 2 [-1  0] mids\n",
            "0.14625000000000002 0.21437500000000004 min, max\n",
            "0.18031250000000004 2 [-1  0] mids\n",
            "0.14625000000000002 0.18031250000000004 min, max\n",
            "0.16328125000000004 2 [-1  0] mids\n",
            "0.14625000000000002 0.16328125000000004 min, max\n",
            "0.15476562500000002 3 [-1  0  1] mids\n",
            "0.15476562500000002 0.16328125000000004 min, max\n",
            "0.15902343750000003 3 [-1  0  1] mids\n",
            "0.15902343750000003 0.16328125000000004 min, max\n",
            "0.16115234375000004 2 [-1  0] mids\n",
            "0.15902343750000003 0.16115234375000004 min, max\n",
            "0.16008789062500003 3 [-1  0  1] mids\n",
            "0.16008789062500003 0.16115234375000004 min, max\n",
            "0.16062011718750002 2 [-1  0] mids\n",
            "0.16008789062500003 0.16062011718750002 min, max\n",
            "Oversampling for group pair (5, 4): Added 297 synthetic samples in 5.\n",
            "0.555 1 [0] mids\n",
            "0.01 0.555 min, max\n",
            "0.28250000000000003 1 [0] mids\n",
            "0.01 0.28250000000000003 min, max\n",
            "0.14625000000000002 3 [-1  0  1] mids\n",
            "0.14625000000000002 0.28250000000000003 min, max\n",
            "0.21437500000000004 2 [-1  0] mids\n",
            "0.14625000000000002 0.21437500000000004 min, max\n",
            "0.18031250000000004 2 [-1  0] mids\n",
            "0.14625000000000002 0.18031250000000004 min, max\n",
            "0.16328125000000004 3 [-1  0  1] mids\n",
            "0.16328125000000004 0.18031250000000004 min, max\n",
            "0.17179687500000004 3 [-1  0  1] mids\n",
            "0.17179687500000004 0.18031250000000004 min, max\n",
            "0.17605468750000003 3 [-1  0  1] mids\n",
            "0.17605468750000003 0.18031250000000004 min, max\n",
            "0.17818359375000004 2 [-1  0] mids\n",
            "0.17605468750000003 0.17818359375000004 min, max\n",
            "0.17711914062500003 2 [-1  0] mids\n",
            "0.17605468750000003 0.17711914062500003 min, max\n",
            "0.17658691406250004 3 [-1  0  1] mids\n",
            "0.17658691406250004 0.17711914062500003 min, max\n",
            "Oversampling for group pair (7, 6): Added 569 synthetic samples in 7.\n",
            "{'name': 'White Male', 'attributes': {'race': 1, 'sex': 1}} {'name': 'Black Male', 'attributes': {'race': 0, 'sex': 1}} gruppi\n",
            "{'name': 'White Male', 'attributes': {'race': 1, 'sex': 1}} {'name': 'White Female', 'attributes': {'race': 1, 'sex': 0}} gruppi\n",
            "{'name': 'White Male', 'attributes': {'race': 1, 'sex': 1}} {'name': 'Black Female', 'attributes': {'race': 0, 'sex': 0}} gruppi\n",
            "{'name': 'Black Male', 'attributes': {'race': 0, 'sex': 1}} {'name': 'White Female', 'attributes': {'race': 1, 'sex': 0}} gruppi\n",
            "{'name': 'Black Male', 'attributes': {'race': 0, 'sex': 1}} {'name': 'Black Female', 'attributes': {'race': 0, 'sex': 0}} gruppi\n",
            "{'name': 'White Female', 'attributes': {'race': 1, 'sex': 0}} {'name': 'Black Female', 'attributes': {'race': 0, 'sex': 0}} gruppi\n",
            "Results for none method:\n",
            "                              Balanced Accuracy  Accuracy  F1 Score  \\\n",
            "Comparison                                                            \n",
            "Black Male vs Black Female              0.63632  0.796368   0.43699   \n",
            "Black Male vs White Female              0.63632  0.796368   0.43699   \n",
            "White Female vs Black Female            0.63632  0.796368   0.43699   \n",
            "White Male vs Black Female              0.63632  0.796368   0.43699   \n",
            "White Male vs Black Male                0.63632  0.796368   0.43699   \n",
            "White Male vs White Female              0.63632  0.796368   0.43699   \n",
            "\n",
            "                              Disparate Impact Ratio  Average Odds Difference  \\\n",
            "Comparison                                                                      \n",
            "Black Male vs Black Female                  0.185296                -0.063097   \n",
            "Black Male vs White Female                  0.417668                -0.059258   \n",
            "White Female vs Black Female                0.438769                -0.003839   \n",
            "White Male vs Black Female                  0.080706                -0.121975   \n",
            "White Male vs Black Male                    0.435736                -0.058878   \n",
            "White Male vs White Female                  0.181981                -0.118136   \n",
            "\n",
            "                              Equal Opportunity Difference  \\\n",
            "Comparison                                                   \n",
            "Black Male vs Black Female                       -0.107636   \n",
            "Black Male vs White Female                       -0.106670   \n",
            "White Female vs Black Female                     -0.000966   \n",
            "White Male vs Black Female                       -0.174530   \n",
            "White Male vs Black Male                         -0.066894   \n",
            "White Male vs White Female                       -0.173564   \n",
            "\n",
            "                                       Consistency  \n",
            "Comparison                                          \n",
            "Black Male vs Black Female    [0.9529134466769706]  \n",
            "Black Male vs White Female    [0.9529134466769706]  \n",
            "White Female vs Black Female  [0.9529134466769706]  \n",
            "White Male vs Black Female    [0.9529134466769706]  \n",
            "White Male vs Black Male      [0.9529134466769706]  \n",
            "White Male vs White Female    [0.9529134466769706]  \n",
            "Results for custom_smote_dbscan method:\n",
            "                              Balanced Accuracy  Accuracy  F1 Score  \\\n",
            "Comparison                                                            \n",
            "Black Male vs Black Female             0.644419  0.775502  0.459488   \n",
            "Black Male vs White Female             0.644419  0.775502  0.459488   \n",
            "White Female vs Black Female           0.644419  0.775502  0.459488   \n",
            "White Male vs Black Female             0.644419  0.775502  0.459488   \n",
            "White Male vs Black Male               0.644419  0.775502  0.459488   \n",
            "White Male vs White Female             0.644419  0.775502  0.459488   \n",
            "\n",
            "                              Disparate Impact Ratio  Average Odds Difference  \\\n",
            "Comparison                                                                      \n",
            "Black Male vs Black Female                  0.719781                -0.011711   \n",
            "Black Male vs White Female                  1.022520                 0.002647   \n",
            "White Female vs Black Female                0.699440                -0.014358   \n",
            "White Male vs Black Female                  0.435917                -0.041930   \n",
            "White Male vs Black Male                    0.598533                -0.030219   \n",
            "White Male vs White Female                  0.614411                -0.027572   \n",
            "\n",
            "                              Equal Opportunity Difference  \\\n",
            "Comparison                                                   \n",
            "Black Male vs Black Female                       -0.034944   \n",
            "Black Male vs White Female                       -0.026132   \n",
            "White Female vs Black Female                     -0.008812   \n",
            "White Male vs Black Female                       -0.047461   \n",
            "White Male vs Black Male                         -0.012517   \n",
            "White Male vs White Female                       -0.038648   \n",
            "\n",
            "                                       Consistency  \n",
            "Comparison                                          \n",
            "Black Male vs Black Female    [0.9270865533230285]  \n",
            "Black Male vs White Female    [0.9270865533230285]  \n",
            "White Female vs Black Female  [0.9270865533230285]  \n",
            "White Male vs Black Female    [0.9270865533230285]  \n",
            "White Male vs Black Male      [0.9270865533230285]  \n",
            "White Male vs White Female    [0.9270865533230285]  \n",
            "Results for custom_smote_dbscan_ratio method:\n",
            "                              Balanced Accuracy  Accuracy  F1 Score  \\\n",
            "Comparison                                                            \n",
            "Black Male vs Black Female              0.64047  0.781646   0.45029   \n",
            "Black Male vs White Female              0.64047  0.781646   0.45029   \n",
            "White Female vs Black Female            0.64047  0.781646   0.45029   \n",
            "White Male vs Black Female              0.64047  0.781646   0.45029   \n",
            "White Male vs Black Male                0.64047  0.781646   0.45029   \n",
            "White Male vs White Female              0.64047  0.781646   0.45029   \n",
            "\n",
            "                              Disparate Impact Ratio  Average Odds Difference  \\\n",
            "Comparison                                                                      \n",
            "Black Male vs Black Female                  0.724633                 0.002250   \n",
            "Black Male vs White Female                  0.939360                -0.013894   \n",
            "White Female vs Black Female                0.771307                 0.016144   \n",
            "White Male vs Black Female                  0.469799                -0.008673   \n",
            "White Male vs Black Male                    0.641187                -0.010923   \n",
            "White Male vs White Female                  0.602355                -0.024817   \n",
            "\n",
            "                              Equal Opportunity Difference  \\\n",
            "Comparison                                                   \n",
            "Black Male vs Black Female                       -0.008628   \n",
            "Black Male vs White Female                       -0.052508   \n",
            "White Female vs Black Female                      0.043880   \n",
            "White Male vs Black Female                        0.004369   \n",
            "White Male vs Black Male                          0.012997   \n",
            "White Male vs White Female                       -0.039511   \n",
            "\n",
            "                                       Consistency  \n",
            "Comparison                                          \n",
            "Black Male vs Black Female    [0.9335780525502313]  \n",
            "Black Male vs White Female    [0.9335780525502313]  \n",
            "White Female vs Black Female  [0.9335780525502313]  \n",
            "White Male vs Black Female    [0.9335780525502313]  \n",
            "White Male vs Black Male      [0.9335780525502313]  \n",
            "White Male vs White Female    [0.9335780525502313]  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#comparison_key = 'Male Adult vs Female Young'\n",
        "#comparison_key = 'Caucasian Female vs Black Male'\n",
        "comparison_key = 'White Male vs Black Female'\n",
        "\n",
        "# Create an empty list to store the extracted data\n",
        "data = []\n",
        "\n",
        "# Iterate over the results dictionary and extract the relevant data\n",
        "for method, result_dict in results['ModelEvaluator'].items():\n",
        "    if comparison_key in result_dict.index:\n",
        "        metrics = result_dict.loc[comparison_key]\n",
        "        row = {\n",
        "            'Classifier': model,\n",
        "            'Technique': method,\n",
        "            'DI Ratio': metrics['Disparate Impact Ratio'],\n",
        "            'AEO Diff.': metrics['Average Odds Difference'],\n",
        "            'Equal Opportunity Difference': metrics['Equal Opportunity Difference'],\n",
        "            'Consis.': metrics['Consistency'],\n",
        "            'Acc.': metrics['Accuracy'],\n",
        "            'Bal. Acc.': metrics['Balanced Accuracy'],\n",
        "            'F1 Score': metrics['F1 Score']\n",
        "        }\n",
        "        data.append(row)\n",
        "\n",
        "# Convert the extracted data into a DataFrame\n",
        "df_results = pd.DataFrame(data)\n",
        "\n",
        "# Print the DataFrame as a formatted table\n",
        "print(df_results.to_string(index=False))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wAtcZjn4SaHh",
        "outputId": "be2d0b76-7037-4397-d368-917a6ea40af8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         Classifier                 Technique  DI Ratio  AEO Diff.  Equal Opportunity Difference              Consis.     Acc.  Bal. Acc.  F1 Score\n",
            "Logistic Regression                      none  0.080706  -0.121975                     -0.174530 [0.9529134466769706] 0.796368   0.636320  0.436990\n",
            "Logistic Regression       custom_smote_dbscan  0.435917  -0.041930                     -0.047461 [0.9270865533230285] 0.775502   0.644419  0.459488\n",
            "Logistic Regression custom_smote_dbscan_ratio  0.469799  -0.008673                      0.004369 [0.9335780525502313] 0.781646   0.640470  0.450290\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "r4tQwNkFhRLH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_group_ratios(X_train_resampled_tax, sensitive_attributes, label):\n",
        "    results = []\n",
        "    groups = X_train_resampled_tax.groupby(sensitive_attributes)\n",
        "\n",
        "    for group_values, group_df in groups:\n",
        "        positive_count = group_df[group_df[label] == 1].shape[0]\n",
        "        negative_count = group_df[group_df[label] == 0].shape[0]\n",
        "\n",
        "        if negative_count == 0:\n",
        "            current_ratio = float('inf')\n",
        "        else:\n",
        "            current_ratio = positive_count / negative_count\n",
        "\n",
        "        # Calculate the number of samples to add to match the total_ratio\n",
        "        # if current_ratio < total_ratio:\n",
        "        #     required_positives = int((total_ratio * negative_count) - positive_count)\n",
        "        #     samples_to_add = max(0, required_positives)  # Ensure no negative numbers\n",
        "        # else:\n",
        "        #     samples_to_add = 0\n",
        "\n",
        "        results.append({\n",
        "            \"group\": group_values,\n",
        "            \"current_ratio\": current_ratio,\n",
        "            #\"samples_to_add\": samples_to_add\n",
        "        })\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "group_ratios = compute_group_ratios(X_train_resampled_dbscan, sensitive_attributes, label)\n",
        "\n",
        "# Display the results\n",
        "for result in group_ratios:\n",
        "    print(f\"Group: {result['group']}, Current Ratio: {result['current_ratio']}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g_V0138gdxIj",
        "outputId": "bc2d9273-8aa0-4ca1-8e42-91f6bd2823ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Group: (0, 0), Current Ratio: 0.4790803159458825\n",
            "Group: (0, 1), Current Ratio: 0.4790803159458825\n",
            "Group: (1, 0), Current Ratio: 0.4790803159458825\n",
            "Group: (1, 1), Current Ratio: 0.4790803159458825\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "group_ratios_ratio = compute_group_ratios(X_train_resampled_dbscan_ratio, sensitive_attributes, label)\n",
        "\n",
        "# Display the results\n",
        "for result in group_ratios_ratio:\n",
        "    print(f\"Group: {result['group']}, Current Ratio: {result['current_ratio']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8R31ZDEcn6SI",
        "outputId": "5bae7aec-a50d-4679-98f6-4fd4e38b43f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Group: (0, 0), Current Ratio: 0.47921225382932164\n",
            "Group: (0, 1), Current Ratio: 0.47983539094650207\n",
            "Group: (1, 0), Current Ratio: 0.4791067269488971\n",
            "Group: (1, 1), Current Ratio: 0.4790803159458825\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "grouped_counts = X_train_resampled_dbscan.groupby(sensitive_attributes + [label]).size().reset_index(name='count')\n",
        "\n",
        "# Print the counts\n",
        "print(grouped_counts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xar_yLJqHVXR",
        "outputId": "d0af9f56-74b5-430a-af28-9eebe6853a6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   race  sex  income  count\n",
            "0     0    0     0.0  12787\n",
            "1     0    0     1.0   6126\n",
            "2     0    1     0.0  12787\n",
            "3     0    1     1.0   6126\n",
            "4     1    0     0.0  12787\n",
            "5     1    0     1.0   6126\n",
            "6     1    1     0.0  12787\n",
            "7     1    1     1.0   6126\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "grouped_counts = X_train_resampled_dbscan_ratio.groupby(sensitive_attributes + [label]).size().reset_index(name='count')\n",
        "\n",
        "# Print the counts\n",
        "print(grouped_counts)"
      ],
      "metadata": {
        "id": "puf3MpGIkboS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23164435-1981-401b-81fb-5f40fcce99bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   race  sex  income  count\n",
            "0     0    0     0.0   1371\n",
            "1     0    0     1.0    657\n",
            "2     0    1     0.0   1215\n",
            "3     0    1     1.0    583\n",
            "4     1    0     0.0   7299\n",
            "5     1    0     1.0   3497\n",
            "6     1    1     0.0  12787\n",
            "7     1    1     1.0   6126\n"
          ]
        }
      ]
    }
  ]
}